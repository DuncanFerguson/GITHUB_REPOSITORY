{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_02_Duncan_Ferguson.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "505kQxu_1ONK",
        "uma7XtwV2PE1",
        "6KNVGaFh2SIw",
        "S9MfIXg05NwR",
        "_MC308fh5q2_"
      ],
      "authorship_tag": "ABX9TyOmdnaBtJa9F0LQkna0QTET",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DuncanFerguson/GITHUB_REPOSITORY/blob/main/Project_02_Duncan_Ferguson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "# Importing Sklearn libraries\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
        "\n",
        "# nltk Libraries\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "!pip install pyprind\n",
        "import pyprind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DdJzJ1hzY0S",
        "outputId": "b0cd0fbc-16c3-4c93-b11e-4afefc627dcf"
      },
      "execution_count": 439,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.7/dist-packages (2.11.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objectives\n",
        "- 1). Read in a classifiable training datset from the Internet using Python for sentiment analysis\n",
        "\n",
        "- 2). Understand the ethical implications of the dataset I've just identified, both in terms of the legality of its use, and in regards to any inherent biases present in the training data.\n",
        "\n",
        "- 3). Preprocessing the training dataset to clean unwanted text data, and extract n-grams from the eample documents that can be used for model training.\n",
        "\n",
        "- 4). Train a logistic regresion model using the cleaned training data that can be used to predict the sentiment of the unclassified examples\n",
        "\n",
        "- 5). Use a grid search to optimize the logistic regression model.\n",
        "\n",
        "- 6). Deploy the optimized and trained logistic regression model to the web using Python pickling, and provide a web form for users to further incrementally train your model by entering additional sample data and providing feedback about its classification accuracy."
      ],
      "metadata": {
        "id": "505kQxu_1ONK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment Objective"
      ],
      "metadata": {
        "id": "uma7XtwV2PE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Describe the dataset I am analyzing, including the TOS and ethical considerations discussed in Lecture 07 & 08 \n",
        "\n",
        "The Data set that is being examined is from the University of California -Irvine's Machine Learning repository. The reason that I chose this data set is because it easy to come by. Last project I wasted two days looking for data sets. Now That I know UCI is easy to extract, I just chose one of their data sets so I could spend more time learning the code and less time data engineering a data set into google colab.\n",
        "\n",
        "This sources is a well known public data set that is used for machine learning. By choosing to go with a public library I am not worring if I am violating a term of agreement. \n",
        "\n",
        "The Proper cite from the readme is below.\n",
        "'This dataset was created for the Paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015'\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
        "\n",
        "- Why did I choose the training examples I did for training my model?\n",
        "\n",
        "I chose the amazon data set for the training model for a few reasons. First there was an even amount of positive and negative reviews. Secondly, I feel as though amazon has a good imput for how different products are recieved. \n",
        "\n",
        "\n",
        "- How is sentiment determined for the training data?\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "6KNVGaFh2SIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "UE7CkbxZ2mCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Am I accesing an API? Scaping a website? Downloading from an archive?\n",
        "\n",
        "The data is being downloaded directly from an archive using !weget from google colab. This will grab the data straight from the source and place it straigt on your G-druve. URL below. https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
        "\n",
        "\n",
        "- Write the necessary Python code to retrieve and store the training examples to create the model.\n",
        "- The resulting implementation should be an in-memory pandas dataframe"
      ],
      "metadata": {
        "id": "zxaImevr2oFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grabbing the websit\n",
        "!wget 'sentiment_labelled_sentences' 'https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP1P55QDyBG-",
        "outputId": "8f124e76-2495-4f09-d004-4b3b0b9575a9"
      },
      "execution_count": 440,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-05 07:20:32--  http://sentiment_labelled_sentences/\n",
            "Resolving sentiment_labelled_sentences (sentiment_labelled_sentences)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘sentiment_labelled_sentences’\n",
            "--2022-08-05 07:20:32--  https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84188 (82K) [application/x-httpd-php]\n",
            "Saving to: ‘sentiment labelled sentences.zip.10’\n",
            "\n",
            "\r          sentiment   0%[                    ]       0  --.-KB/s               \rsentiment labelled  100%[===================>]  82.21K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-08-05 07:20:32 (3.80 MB/s) - ‘sentiment labelled sentences.zip.10’ saved [84188/84188]\n",
            "\n",
            "FINISHED --2022-08-05 07:20:32--\n",
            "Total wall clock time: 0.1s\n",
            "Downloaded: 1 files, 82K in 0.02s (3.80 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzipping the file\n",
        "!unzip -o 'sentiment labelled sentences.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uim66ZPuyQX_",
        "outputId": "d8dfc8fa-7927-4999-e822-fa5b2d3bc221"
      },
      "execution_count": 441,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  sentiment labelled sentences.zip\n",
            "  inflating: sentiment labelled sentences/.DS_Store  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._.DS_Store  \n",
            "  inflating: sentiment labelled sentences/amazon_cells_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/imdb_labelled.txt  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._imdb_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/readme.txt  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._readme.txt  \n",
            "  inflating: sentiment labelled sentences/yelp_labelled.txt  \n",
            "  inflating: __MACOSX/._sentiment labelled sentences  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the data in a dataframe\n",
        "df = pd.read_csv('sentiment labelled sentences/amazon_cells_labelled.txt', sep='\\t', names=['review', 'Sentiment'])\n",
        "# df2 = pd.read_csv('sentiment labelled sentences/imdb_labelled.txt', sep='\\t', names=['review', 'Sentiment'])\n",
        "df.head()\n",
        "df_clean = df.copy()"
      ],
      "metadata": {
        "id": "O-CjlFapyxfV"
      },
      "execution_count": 442,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Saving this nice little dataset as a an excel file\n",
        "from google.colab import files\n",
        "df.to_csv('amazon_sentiment.csv', encoding = 'utf-8', index=False)\n",
        "files.download('amazon_sentiment.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TzfIrRN71HpQ",
        "outputId": "7478feb2-a973-4c6c-9076-c3441554a230"
      },
      "execution_count": 500,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_56c95c77-d08a-4838-a00c-56376c0da350\", \"amazon_sentiment.csv\", 58726)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "C94Nxv7K25O9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This is where the preprocessing utility for the project will be built.\n",
        "\n",
        "- Steps required, clean text data, tokenize the document, construct a TfidfVectorizer to be performed here"
      ],
      "metadata": {
        "id": "2xSohlCh267V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the Preprocessing of regular expresions from ch08\n",
        "def preprocessor(text):\n",
        "  \"\"\"This preprocessing function removes all the punctuation, makes the word\n",
        "  lower case, removews the stop words, and porterStems the words\"\"\"\n",
        "  # Removing all the punctuation\n",
        "  text = re.sub('<[^>]*>', '', text)\n",
        "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "  text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')).strip()\n",
        "  return text\n",
        "\n",
        "# NOt that the tokenizing is set up for a pipeline run. \n",
        "def tokenizer(text):\n",
        "  \"\"\"Setting up the tokenizer ie split\"\"\"\n",
        "  return text.split()\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "  \"\"\"Tokenizing and porting\"\"\"\n",
        "  porter = PorterStemmer()\n",
        "  return [porter.stem(word) for word in text.split()]\n",
        "\n",
        "def tokenizer_stop(text):\n",
        "  return [word for word in tokenizer_porter(text) if word not in stop]\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "x = df['review'].apply(preprocessor)\n",
        "y = df['Sentiment']\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, \n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    shuffle=True)\n",
        "\n",
        "# constructing a TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(use_idf=True, norm='l2', smooth_idf=True)\n",
        "\n",
        "# Setting up the Param Grid\n",
        "param_grid = [{'vect__ngram_range': [(1,1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_stop],\n",
        "               'clf__penalty': ['l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]},\n",
        "              {'vect__ngram_range': [(1,1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer,tokenizer_porter, tokenizer_stop],\n",
        "               'vect__use_idf':[False],\n",
        "               'vect__norm':[None],\n",
        "               'clf__penalty': ['l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]}]\n",
        "\n",
        "# Setting up the Pipeline\n",
        "pipe = Pipeline(steps=[('vect', tfidf), \n",
        "                       ('clf', LogisticRegression(random_state=42,\n",
        "                                                  solver='liblinear'))])\n",
        "\n",
        "gs_pipe = GridSearchCV(estimator=pipe, param_grid=param_grid,\n",
        "                       scoring='accuracy',\n",
        "                       cv=5,\n",
        "                       verbose=2,\n",
        "                       n_jobs=-1,\n",
        "                       error_score='raise')\n",
        "\n",
        "gs_pipe.fit(x_train.to_list(), y_train.to_list())\n",
        "print('Hyperparameters parameter set: %s ' % gs_pipe.best_params_)\n",
        "print('Accuracy: %s ' % gs_pipe.best_score_)"
      ],
      "metadata": {
        "id": "N7PbUcC70vJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c0412d8-52f3-46cf-ad7a-008cf736f953"
      },
      "execution_count": 444,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "Hyperparameters parameter set: {'clf__C': 100.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__norm': None, 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer_porter at 0x7f3cff361b90>, 'vect__use_idf': False} \n",
            "Accuracy: 0.8150000000000001 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Optimization and Serialization"
      ],
      "metadata": {
        "id": "6t-EBumNjXdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This is where I will use a grid search similar to the one performed in lecture 11: Applying Machine Learning to Sentiment Anlysis\n",
        "\n",
        "- Find the optimal hyperparameters (including choice of stemming algorithm for TfidVectorizer) to use with my out-of-core capable logistic regression classifier (like SGDClassifier)  Utilizing the TfidVectorizerI created in tepp three to preprocess my data\n",
        "\n",
        "The Code for finding the hyperparameres and the Grid Search are right above"
      ],
      "metadata": {
        "id": "XOxClVd8janD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_docs(path):\n",
        "  with open(path, 'r', encoding='utf-8', newline='\\n') as csv:\n",
        "    next(csv) # skipping the header\n",
        "    for line in csv:\n",
        "      text, label = line[:-3], line[:-1][-1]\n",
        "      yield text, label\n",
        "\n",
        "display(next(stream_docs(path='amazon_sentiment.csv')))\n",
        "# Displaying that Next Works\n",
        "# for _ in range(10):\n",
        "#   display(next(stream_docs(path='amazon_sentiment.csv')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "JWjmyHHgyD3v",
        "outputId": "567f0a94-0c57-4c7d-fd7b-bfd2ff5ee30b"
      },
      "execution_count": 507,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
              " '0')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
              " '0')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
              " '0')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
              " '0')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
              " '0')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
              " '0')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
              " '0')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
              " '0')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
              " '0')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
              " '0')"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minibatch(doc_stream, size):\n",
        "  docs, y = [], []\n",
        "  try:\n",
        "    for _ in range(size):\n",
        "      text, label = next(doc_stream)\n",
        "      docs.append(text)\n",
        "      y.append(label)\n",
        "  except StopIteration:\n",
        "    return None, None\n",
        "  return docs, y\n",
        "\n",
        "vect = HashingVectorizer(decode_error='ignore',\n",
        "                         n_features=2**21,\n",
        "                         preprocessor=preprocessor,\n",
        "                         tokenizer=tokenizer_stop)\n",
        "\n",
        "clf = SGDClassifier(loss='log', random_state=42)\n",
        "\n",
        "doc_stream = stream_docs(path='amazon_sentiment.csv')"
      ],
      "metadata": {
        "id": "kaM0v38iydJF"
      },
      "execution_count": 490,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pbar = pyprind.ProgBar(10)\n",
        "classes = np.array([0,1])\n",
        "\n",
        "for _ in range(45):\n",
        "  x_train, y_train = get_minibatch(doc_stream, size=100) # Change size to be larger later on\n",
        "  # if not x_train:\n",
        "  #   break\n",
        "  # display(y_train)\n",
        "  # x_train = vect.transform(x_train)\n",
        "  # clf.partial_fit(x_train, y_train.values.ravel(), classes=classes)\n",
        "  # pbar.update()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "id": "ZZqAJQm9tUzG",
        "outputId": "6578d60f-a551-4b55-98d5-7dafea90b7ec"
      },
      "execution_count": 494,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test = get_minibatch(doc_stream, size=5000)\n",
        "display(x_test)\n",
        "# x_test = vect.transform(x_test)\n",
        "# print('Accuracy: %.3f' % clf.score(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CSl14qjut4rA",
        "outputId": "a493bce3-9c45-495a-f981-35e8e4525c26"
      },
      "execution_count": 492,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Document the results of the grid seach in a marckdown cell immediately after the grid search, including computed accuracies agianst my training and test data sets.\n",
        "\n",
        "- After documenting the finding fit the classifier against my entire dataset, then pickle the resulting object to a file that will be used in the website\n",
        "\n",
        "* Note: Out-of-core learning is a requirement. Since I will be updating the model using the deployued Flask application, the classifier I pickle for yuse on the website must support incremental learning\n",
        "\n",
        "* Note the hyperparamters I explore in the grid search must include different values than we used in class. One of the goals of this project is to see if other options migh yield better results than the book attempted. \n",
        "\n",
        "- Be sure to exaplain in the write up for this section why I chose the values that I did for the hyperparameters, and if they performed better for my data set than the other ones the booke attempted to use\n",
        "\n",
        "* NOte: The grid search I perform will be agianst an something like an SGDClassifier instance, instead of the LogisticRegrssion classifier that was used in the book if you use SFDClassifer, instead of varying the value of C for regularization, you will vary the values for alpha. Still compare l1 vs l2 regulatization. This will result in a comparison of my work to the example in the book that isnt' strictly 1-1 but will be close enough to draw meaningful conculsions against\n"
      ],
      "metadata": {
        "id": "VJcWoh1J3a8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7J8IRkxDIWaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "azNfxtbzi32J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creat a standalone Python file"
      ],
      "metadata": {
        "id": "S9MfIXg05NwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- File shoudl include all of the data preprocessing functionality created in step 4 named vectorizer.py. This file will initialize a HashingVectorizer using the parameters I learned tuning TfidVectorizer in step Four\n",
        "- The creation of vectorizer.py should be done inside project02,pynb file similiar to ch09.ipynb"
      ],
      "metadata": {
        "id": "yvCbjMJy5TtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Website Creation and Publishing"
      ],
      "metadata": {
        "id": "_MC308fh5q2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create an interactive website using Flask, SQlite, WTforms, and Jinja2, that will enable me to test the accuracy of my calculated model by prompting users to enter text in the same form as your train data (product reviews, social media posts, etc. ) and perform sentiment analysis against their submissionts\n",
        "\n",
        "- Very similar to Lecture 12 -Embedding a Machine Learning Model into a Web Application, I will then show the user my sentiment prediction base on their entry, allowing them to rate whether the prediction was accurate or not. \n",
        "\n",
        "- User provided examples will be stored in SQLite and be used to incrementally refine the trained model I created in step 4. \n",
        "\n",
        "- Complete website will be a published to PthonAnywhere and a link to the website will be included in a markdown cell in this Jupter notebook.\n",
        "\n",
        "- The source code for the website (including Python source code, HTML templates, and Python pickle files) will be submitted alongside the Jupyter Notbook in Goodle Drive \n",
        "\n",
        "- Note: I must use the pickled model created in step four as the starting point for my classifier. In addition, if your webserver was to be restared, all user-provided examples must still represet in the trained model (i.e. I cannot rest the pickled version only on server restart)\n",
        "\n",
        "- Note: Make sure to look closely at the code thats being reused for the Flask application to make sure that assumptions built into the existing app still hold for the dataset. For instance, make sure that the handling of correct/incorrect feedback via the website works for my data."
      ],
      "metadata": {
        "id": "g3lWEFJY5ul8"
      }
    }
  ]
}