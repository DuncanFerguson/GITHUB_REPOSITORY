{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_02_Duncan_Ferguson_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "505kQxu_1ONK"
      ],
      "authorship_tag": "ABX9TyPNow8YFbYspShoDcvSJ1S+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DuncanFerguson/GITHUB_REPOSITORY/blob/main/Project_02_Duncan_Ferguson_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Importing Sklearn libraries\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, StratifiedKFold\n",
        "from distutils.version import LooseVersion as Version\n",
        "from sklearn import __version__ as sklearn_version\n",
        "\n",
        "# nltk Libraries\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "!pip install pyprind\n",
        "import pyprind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DdJzJ1hzY0S",
        "outputId": "88c83af4-4ee9-4a16-c323-0017780e393c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.7/dist-packages (2.11.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objectives\n",
        "- 1). Read in a classifiable training datset from the Internet using Python for sentiment analysis\n",
        "\n",
        "- 2). Understand the ethical implications of the dataset I've just identified, both in terms of the legality of its use, and in regards to any inherent biases present in the training data.\n",
        "\n",
        "- 3). Preprocessing the training dataset to clean unwanted text data, and extract n-grams from the eample documents that can be used for model training.\n",
        "\n",
        "- 4). Train a logistic regresion model using the cleaned training data that can be used to predict the sentiment of the unclassified examples\n",
        "\n",
        "- 5). Use a grid search to optimize the logistic regression model.\n",
        "\n",
        "- 6). Deploy the optimized and trained logistic regression model to the web using Python pickling, and provide a web form for users to further incrementally train your model by entering additional sample data and providing feedback about its classification accuracy."
      ],
      "metadata": {
        "id": "505kQxu_1ONK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment Objective"
      ],
      "metadata": {
        "id": "uma7XtwV2PE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective 2). Understand the ethical implications of the dataset I've just identified, both in terms of the legality of its use, and in regards to any inherent biases present in the training data. \n",
        "\n",
        "- Objective 2 detailed below.\n",
        "\n",
        "### Describe the dataset I am analyzing, including the TOS and ethical considerations discussed in Lecture 07 & 08 \n",
        "\n",
        "- The Data set that is being examined is from the University of California - Irvine's Machine Learning repository. The reason that I chose this data set is because it easy to come by. Last project I wasted two days looking for data sets. Now That I know UCI is easy to extract, I just chose one of their data sets so I could spend more time learning the code and less time data engineering a data set into google colab. After going through the excersise I wish I had chosen a data set that had more fields.\n",
        "\n",
        "- This sources is a well known public data set that is used for machine learning. By choosing to go with a public library I am not worring if I am violating a term of agreement. \n",
        "\n",
        "- The Proper cite from the readme is below.\n",
        "'This dataset was created for the Paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015'\n",
        "\n",
        "- https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
        "\n",
        "### Why did I choose the training examples I did for training my model?\n",
        "\n",
        "- I chose the amazon data set for the training model for a few reasons. First there was an even amount of positive and negative reviews. Secondly, I feel as though amazon has a good imput for how different products are recieved. This data can have inherent biases from amazong customers in the training data. There might be some reviews that are input by people that simply don't like a supplier. There are other bits that can go into the bias of the training data set such as taking out the stop words and how many Ngrams are used or the combination of the words. Other concerns are how small the data set is.\n",
        "\n",
        "## How is sentiment determined for the training data?\n",
        "\n",
        "- Good sentiment is determined with a 1 while bad sentiment is determined with a 0. For the first round I chose to split the data in to 80% training and 20% testing.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6KNVGaFh2SIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "UE7CkbxZ2mCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Am I accesing an API? Scaping a website? Downloading from an archive?\n",
        "\n",
        "- The data is being downloaded directly from an archive using !weget from google colab. This will grab the data straight from the source and place it straigt on your G-druve. URL below. https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
        "\n",
        "\n",
        "### Write the necessary Python code to retrieve and store the training examples to create the model.\n",
        "- See below"
      ],
      "metadata": {
        "id": "zxaImevr2oFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grabbing the websit\n",
        "!wget 'sentiment_labelled_sentences' 'https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP1P55QDyBG-",
        "outputId": "6c30691e-b19f-4716-9523-e57f3c1f39db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-07 09:34:53--  http://sentiment_labelled_sentences/\n",
            "Resolving sentiment_labelled_sentences (sentiment_labelled_sentences)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘sentiment_labelled_sentences’\n",
            "--2022-08-07 09:34:53--  https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84188 (82K) [application/x-httpd-php]\n",
            "Saving to: ‘sentiment labelled sentences.zip.8’\n",
            "\n",
            "sentiment labelled  100%[===================>]  82.21K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-08-07 09:34:54 (835 KB/s) - ‘sentiment labelled sentences.zip.8’ saved [84188/84188]\n",
            "\n",
            "FINISHED --2022-08-07 09:34:54--\n",
            "Total wall clock time: 0.4s\n",
            "Downloaded: 1 files, 82K in 0.1s (835 KB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzipping the file\n",
        "!unzip -o 'sentiment labelled sentences.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uim66ZPuyQX_",
        "outputId": "9c2fa86b-ca3d-4edb-c4f6-84e1974a13ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  sentiment labelled sentences.zip\n",
            "  inflating: sentiment labelled sentences/.DS_Store  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._.DS_Store  \n",
            "  inflating: sentiment labelled sentences/amazon_cells_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/imdb_labelled.txt  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._imdb_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/readme.txt  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._readme.txt  \n",
            "  inflating: sentiment labelled sentences/yelp_labelled.txt  \n",
            "  inflating: __MACOSX/._sentiment labelled sentences  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective 1). Read in a classifiable training datset from the Internet using Python for sentiment analysis:\n",
        "\n",
        "### The resulting implementation should be an in-memory pandas dataframe\n",
        "- Below is the display of the dataframe head"
      ],
      "metadata": {
        "id": "G8Yo3mFGMquF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the data in a dataframe\n",
        "df = pd.read_csv('sentiment labelled sentences/amazon_cells_labelled.txt', sep='\\t', names=['review', 'Sentiment'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "O-CjlFapyxfV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "baa018ef-0f7d-4136-cbaa-4d19c5602217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  Sentiment\n",
              "0  So there is no way for me to plug it in here i...          0\n",
              "1                        Good case, Excellent value.          1\n",
              "2                             Great for the jawbone.          1\n",
              "3  Tied to charger for conversations lasting more...          0\n",
              "4                                  The mic is great.          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f68e725e-2417-4bef-88b5-439f1dceb80f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f68e725e-2417-4bef-88b5-439f1dceb80f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f68e725e-2417-4bef-88b5-439f1dceb80f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f68e725e-2417-4bef-88b5-439f1dceb80f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Code below is commented out, but was used to push the data set into a csv for later use or for analysis"
      ],
      "metadata": {
        "id": "jcooVqfsM7Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving this nice little dataset as a an excel file\n",
        "from google.colab import files\n",
        "df.to_csv('amazon_sentiment.csv', encoding = 'utf-8', index=False)\n",
        "files.download('amazon_sentiment.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TzfIrRN71HpQ",
        "outputId": "7e546366-9f4c-4760-cb79-fe3e3e4bc97a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4e970638-0ba7-4c55-8b25-e95ef2d17a95\", \"amazon_sentiment.csv\", 58726)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "C94Nxv7K25O9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective 3). Preprocessing the training dataset to clean unwanted text data, and extract n-grams from the example documents that can be used for model training.\n",
        "  - Completed below\n",
        "\n",
        "### This is where the preprocessing utility for the project will be built.\n",
        "\n",
        "### Steps required, clean text data, tokenize the document, construct a TfidfVectorizer to be performed here\n",
        "- Completed below"
      ],
      "metadata": {
        "id": "2xSohlCh267V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the Preprocessing of regular expresions from ch08\n",
        "def preprocessor(text):\n",
        "  \"\"\"This preprocessing function removes all the punctuation, makes the word\n",
        "  lower case, removews the stop words, and porterStems the words\"\"\"\n",
        "  # Removing all the punctuation\n",
        "  text = re.sub('<[^>]*>', '', text)\n",
        "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "  text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')).strip()\n",
        "  return text\n",
        "\n",
        "# Note that the tokenizing is set up for a pipeline run. \n",
        "def tokenizer(text):\n",
        "  \"\"\"Setting up the tokenizer ie split\"\"\"\n",
        "  return text.split()\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "  \"\"\"Tokenizing and porting\"\"\"\n",
        "  porter = PorterStemmer()\n",
        "  return [porter.stem(word) for word in text.split()]\n",
        "\n",
        "def tokenizer_stop(text):\n",
        "  return [word for word in tokenizer_porter(text) if word not in stop]\n",
        "\n",
        "# constructing a TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(use_idf=True, norm='l2', smooth_idf=True)"
      ],
      "metadata": {
        "id": "N7PbUcC70vJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Optimization and Serialization"
      ],
      "metadata": {
        "id": "6t-EBumNjXdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is where I will use a grid search similar to the one performed in lecture 11: Applying Machine Learning to Sentiment Anlysis\n",
        "\n",
        "### Find the optimal hyperparameters (including choice of stemming algorithm for TfidVectorizer) to use with my out-of-core capable logistic regression classifier (like SGDClassifier)  Utilizing the TfidVectorizerI created in step three to preprocess my data\n",
        "\n",
        "- The Code for finding the hyperparameres and the Grid Search are right below"
      ],
      "metadata": {
        "id": "XOxClVd8janD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop = stopwords.words('english')\n",
        "\n",
        "# Splitting the data in to x y\n",
        "x = df['review']\n",
        "y = df['Sentiment']\n",
        "\n",
        "# Splitting the data up in to training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, \n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    shuffle=True)\n",
        "\n",
        "# Model optimization and serialization\n",
        "# Setting up the Param Grid\n",
        "param_grid = [{'vect__ngram_range': [(1,1), (1,2)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__preprocessor':[preprocessor],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_stop],\n",
        "               'vect__use_idf':[False, True],\n",
        "               'vect__norm':['l1', 'l2', None],\n",
        "               'vect__smooth_idf':[True],\n",
        "               'clf__loss': ['log'],\n",
        "               'clf__penalty': ['l1','l2'],\n",
        "               'clf__alpha': [0.001, 0.1, 1.0]},\n",
        "              {'vect__ngram_range': [(1,1), (1,2)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__preprocessor':[preprocessor],\n",
        "               'vect__tokenizer': [tokenizer,tokenizer_porter, tokenizer_stop],\n",
        "               'vect__use_idf':[False, True],\n",
        "               'vect__smooth_idf':[True],\n",
        "               'vect__norm':['l1', 'l2', None],\n",
        "               'clf__loss': ['log'],\n",
        "               'clf__penalty': ['l1', 'l2'],\n",
        "               'clf__alpha': [0.001, 0.1, 1.0]}]"
      ],
      "metadata": {
        "id": "3dxOyW5twasu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective 4). Train a logistic regresion model using the cleaned training data that can be used to predict the sentiment of the unclassified examples\n",
        "\n",
        "- Completed Below\n",
        "\n",
        "### Objective 5). Use a grid search to optimize the logistic regression model.\n",
        "\n",
        "- Completed below\n",
        "\n",
        "### Objective: Note the hyperparameters I explore in the grid search must include different values than we used in class. One of the goals of this project is to see if  other options migh yield better results than the book attempted. \n",
        "\n",
        "### Objective: Out-of-core learning is a requirement. Since I will be updating my model using the deployed Flask application, the classifier I pickle for use on the website must support incremental learning\n",
        "\n",
        "- SGDClassifier was used, and this is supported by out-of-core learning\n",
        "\n",
        "### Objective: The gridsearch I perform wil be against something like an SGDClassifier instance, instead of the LogisticRegression classifier that was used in the book. Because I used the SGDClassifier I changed the value of C for regularisation  to alpha. L1 and L2 are still compared.\n",
        "\n",
        "\n",
        "- The following paramaters in the param grid have been changed\n",
        "  - vect__ngram_range: [(1,1)]  to [(1,1),(1,2)]\n",
        "  - vect__stop_words: Same\n",
        "  - vect__preprocessor: Has been added\n",
        "  - vect__tokenizer: [(tokenizer, tokenizer_porter)] to [(tokenizer, tokenizer_porter, tokenizer_stop)]\n",
        "  - vect__use_idf: [False] to [False, True]\n",
        "  - vect__smooth_idf: Same\n",
        "  - vect_norm: [None] to ['l1', 'l2', None]\n",
        "  - clf__loss: Same [log]\n",
        "  - clf__plenatly: Same ['l1', 'l2']\n",
        "  - clf__c: Changed to clf_alpha: values changed to [0.01, 0.1, 1.0]\n"
      ],
      "metadata": {
        "id": "dWvpPmOs8jXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the Pipeline\n",
        "pipe = Pipeline(steps=[('vect', tfidf), \n",
        "                       ('clf', SGDClassifier(loss='log', random_state=42))])\n",
        "\n",
        "gs_pipe = GridSearchCV(estimator=pipe, param_grid=param_grid,\n",
        "                       scoring='accuracy',\n",
        "                       cv=5,\n",
        "                       verbose=3,\n",
        "                       n_jobs=-1,\n",
        "                       error_score='raise')\n",
        "\n",
        "gs_pipe.fit(x_train.to_list(), y_train.to_list())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jVHQj3c8iws",
        "outputId": "278acde4-053e-4129-8b60-1680a4bee4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score='raise',\n",
              "             estimator=Pipeline(steps=[('vect', TfidfVectorizer()),\n",
              "                                       ('clf',\n",
              "                                        SGDClassifier(loss='log',\n",
              "                                                      random_state=42))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid=[{'clf__alpha': [0.001, 0.1, 1.0], 'clf__loss': ['log'],\n",
              "                          'clf__penalty': ['l1', 'l2'],\n",
              "                          'vect__ngram_range': [(1, 1), (1, 2)],\n",
              "                          'vect__norm': ['l1', 'l2', None],\n",
              "                          'vect__preprocessor': [<function preprocessor a...\n",
              "                                                \"you'll\", \"you'd\", 'your',\n",
              "                                                'yours', 'yourself',\n",
              "                                                'yourselves', 'he', 'him',\n",
              "                                                'his', 'himself', 'she',\n",
              "                                                \"she's\", 'her', 'hers',\n",
              "                                                'herself', 'it', \"it's\", 'its',\n",
              "                                                'itself', ...],\n",
              "                                               None],\n",
              "                          'vect__tokenizer': [<function tokenizer at 0x7fd6f6aebdd0>,\n",
              "                                              <function tokenizer_porter at 0x7fd6f5f39e60>,\n",
              "                                              <function tokenizer_stop at 0x7fd6f5f39dd0>],\n",
              "                          'vect__use_idf': [False, True]}],\n",
              "             scoring='accuracy', verbose=3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the best parameters\n",
        "print('Hyperparameters parameter set: %s ' % gs_pipe.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIZ1Nm3IAhb0",
        "outputId": "f581d20c-52b8-495b-a784-e189a7671f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters parameter set: {'clf__alpha': 0.001, 'clf__loss': 'log', 'clf__penalty': 'l2', 'vect__ngram_range': (1, 2), 'vect__norm': 'l2', 'vect__preprocessor': <function preprocessor at 0x7fd6f65d0440>, 'vect__smooth_idf': True, 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer_porter at 0x7fd6f5f39e60>, 'vect__use_idf': True} \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train Accuracy: %s ' % gs_pipe.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW0Bm5BhHUbh",
        "outputId": "02ce22e3-096f-4408-da89-a0da23fa98e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8137500000000001 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test Accuracy: %.3f' % gs_pipe.score(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYjuNq3zHUmT",
        "outputId": "a7475fa6-4a3a-466b-f3fc-e2706b836f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document the results of the grid seach in a markdown cell immediately after the grid search, including computed accuracies against my training and test data sets.\n",
        "\n",
        "- Train Accuracy: 81.31375%\n",
        "- Test Accuracy: 83.0%\n",
        "\n",
        "The hyper parameters that were chosen are:\n",
        "\n",
        "- For the Vect\n",
        "  - vect__ngram_range: (1,2)\n",
        "  - vect__norm: l2\n",
        "  - vect__preprocessor: Preprocessor\n",
        "  - vect__stop_words: None\n",
        "  - vect__tokenizer: tokenizer_porter\n",
        "  - vect__use_idf: True\n",
        "  - vect__smooth_idf: True\n",
        "\n",
        "- For CLF\n",
        "  - clf_loss: log\n",
        "  - clf__alpha: 0.01\n",
        "  - clf__penalty: l2\n",
        "\n",
        "\n",
        "### Be sure to exaplain in the write up for this section why I chose the values that I did for the hyperparameters, and if they performed better for my data set than the other ones the booke attempted to use\n",
        "\n",
        "- When comparing the train and test accuraries the test actually performed better. \n",
        "- For Ngrams I decided to look at both (1,1) and (1,2). (1,1) only uses unigrams where as (1,2) uses both unigrams and bigrams. Unsprisingly the (1,2) helped the model perform better. this is using combinations of words to help determine the sentiment.\n",
        "- vect__norm: Was set to l2 indicating that regularization that is standard performed bessed.\n",
        "- Preprocessor was used in this case so that the pickling could be conducted easier with out having to clean the data set. This way when we go back to update the model it will always clean the data.\n",
        "- The tokenizer that was used was tokenizer_porter. This tokenizer used stop words and also stemmed the words. \n",
        "- vect__stop_words: This came up as using none in the parameters, This maybe because of the Ngrams using combinations of two words.\n",
        "- Use IDF had the choice of both False or True. In this case True was chosen as making the model better. This means that inverse document frequency reweighting is used.\n",
        "- Vect__smooth: Is unsuprisingly True\n",
        "- When looking at the SGDClassifer the alpha that was chosen was .001. A higher alpha means the migher the regulaization. In this case the least amount of regulaization was chosen.\n",
        "- For the SGDClassifer the best parameter was L2 which is the standard regulizer for linear SVM models. L1 would have brought more sparity to the model\n"
      ],
      "metadata": {
        "id": "DcFYeRJzAXGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After documenting the findings above. I am fitting the classifer against the entire data set and then pickling the resulting object to a file that will be used in the website.\n",
        "\n",
        "* Note: The grid search I perform agaisnt the SGDClassifier instance, instead of the LogisticRegrssion classifier that was used in the book if you use SFDClassifer, instead of varying the value of C for regularization, you will vary the values for alpha. Still compare l1 vs l2 regulatization. This will result in a comparison of my work to the example in the book that isnt' strictly 1-1 but will be close enough to draw meaningful conculsions against"
      ],
      "metadata": {
        "id": "eupNNX7KOCow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some of the code below is re-written to make it easier for the creation of the vectorizer\n",
        "stop = stopwords.words('english')\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def preprocessor(text):\n",
        "  \"\"\"This preprocessing function removes all the punctuation\"\"\"\n",
        "  # Removing all the punctuation\n",
        "  text = re.sub('<[^>]*>', '', text)\n",
        "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "  text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')).strip()\n",
        "  return text\n",
        "\n",
        "# Note that the tokenizing is set up for a pipeline run. \n",
        "def tokenizer_porter(text):\n",
        "  \"\"\"Tokenizing and porting\"\"\"\n",
        "  return [porter.stem(word) for word in text.split()]\n",
        "\n",
        "# Creating a function for streaming\n",
        "def stream_docs(path):\n",
        "  with open(path, 'r', encoding='utf-8', newline='\\n') as csv:\n",
        "    next(csv) # skipping the header\n",
        "    for line in csv:\n",
        "      text, label = line[:-3], line[:-1][-1]\n",
        "      yield text, label\n",
        "\n",
        "def get_minibatch(doc_stream, size):\n",
        "  docs, y = [], []\n",
        "  try:\n",
        "    for _ in range(size):\n",
        "      text, label = next(doc_stream)\n",
        "      docs.append(text)\n",
        "      y.append(int(label))\n",
        "  except StopIteration:\n",
        "    return None, None\n",
        "  return docs, y\n",
        "\n",
        "# Setting the Hashing Vectorize with the params from above\n",
        "vect = HashingVectorizer(decode_error='ignore',\n",
        "                         n_features=2*22,\n",
        "                         norm='l2',\n",
        "                         ngram_range=(1,2),\n",
        "                         preprocessor=preprocessor,\n",
        "                         tokenizer=tokenizer_porter,\n",
        "                         stop_words=stop)\n",
        "\n",
        "# Setting the SGDClassifier with the parms from above\n",
        "clf = SGDClassifier(alpha=0.01, loss='log', penalty='l2', random_state=42)"
      ],
      "metadata": {
        "id": "JWjmyHHgyD3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Setting up Google drive to take in the Dataframe that was extracted straight from the website\n",
        "from google.colab import drive\n",
        "# drive.flush_and_unmount()\n",
        "drive.mount('content/')\n",
        "path = 'amazon_sentiment.csv'\n",
        "\n",
        "with open(path, 'w', encoding = 'utf-8') as f:\n",
        "  df.to_csv(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9MqiX5fdk_B",
        "outputId": "db997135-4c99-4e35-ea28-15b85d63caa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at content/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Showing an example of the mount working\n",
        "next(stream_docs('amazon_sentiment.csv'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDz7Mm3Ef448",
        "outputId": "ff82de06-4b84-4a51-c0f2-fb571eea95ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('0,So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
              " '0')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_stream = stream_docs(path='amazon_sentiment.csv')"
      ],
      "metadata": {
        "id": "YKbGKRyfgJJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyprind\n",
        "pbar = pyprind.ProgBar(10)\n",
        "for _ in range(10):\n",
        "  x_train, y_train = get_minibatch(doc_stream, size=100)\n",
        "  if not x_train:\n",
        "    break\n",
        "  x_train = vect.transform(x_train)\n",
        "  clf.partial_fit(x_train, y_train, classes=np.array([1,0]))\n",
        "  pbar.update()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOlJ-1mfjjMW",
        "outputId": "2f21fe00-189c-4b6e-d7a6-9679ac8f35ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "0% [##########] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy: %.3f' % clf.score(x_train, y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsvqcrWrk1zN",
        "outputId": "04a0326b-5edd-4a1d-8eea-2e60818baae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that when using the minibatching to train the model that the the accuracy went down and is now 63-64% (depending on the run) this is a notable difference from above. But it does show that the vectorizer can be used with out of core learning. This will be what we use in the standalone python file.\n",
        "\n",
        "### Creating the Pickle\n",
        "- Below I will create a pickle on the full data set. Using the out of core model. Though this is not as accurate as using TfidVectorizer it is a requirement for the project."
      ],
      "metadata": {
        "id": "7_vlX9OCpImI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = df['review'].values\n",
        "y_train = df['Sentiment'].values\n",
        "\n",
        "x_train = vect.transform(x_train)\n",
        "clf.fit(x_train, y_train)\n",
        "print('Accuracy: %.3f' % clf.score(x_train, y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3tuJ_F3vFzh",
        "outputId": "b652be9c-3887-4e66-a234-82462db81a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again when training the full dataset using the Hashingvectorizer the accuracy is a bit lower than we would have like than just using the TfidVectorizer. But we'll take it as working and move on toward pickling."
      ],
      "metadata": {
        "id": "hnFd-Mpsx7WL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# drive.flush_and_mount()\n",
        "# drive.flush_and_unmount()\n",
        "# drive.mount('content/', force_remount=True)"
      ],
      "metadata": {
        "id": "DTtZ4NE0wloO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the google drive mount\n",
        "%cd content/MyDrive/ML_Colab/Project_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC9Yiy_8-i7O",
        "outputId": "574e7a42-5c7a-4d24-e232-1c4d37db6abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/content/MyDrive/ML_Colab/Project_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory already made\n",
        "# %mkdir pkl_objects"
      ],
      "metadata": {
        "id": "LFQgPrixDCHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.path\n",
        "!find"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKuEheji9Z2c",
        "outputId": "be7ac619-f3c5-4ba5-b8f7-eb4ce3fc1bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "./Project_02_Duncan_Ferguson_v2.ipynb\n",
            "./__pycache__\n",
            "./__pycache__/vectorizer.cpython-37.pyc\n",
            "./pkl_objects\n",
            "./pkl_objects/classifier.pkl\n",
            "./pkl_objects/stopwords.pkl\n",
            "./static\n",
            "./static/style.css\n",
            "./templates\n",
            "./templates/reviewform.html\n",
            "./templates/results.html\n",
            "./templates/thanks.html\n",
            "./templates/_formhelpers.html\n",
            "./sentiment labelled sentences.zip\n",
            "./sentiment labelled sentences\n",
            "./sentiment labelled sentences/.DS_Store\n",
            "./sentiment labelled sentences/amazon_cells_labelled.txt\n",
            "./sentiment labelled sentences/imdb_labelled.txt\n",
            "./sentiment labelled sentences/readme.txt\n",
            "./sentiment labelled sentences/yelp_labelled.txt\n",
            "./__MACOSX\n",
            "./__MACOSX/sentiment labelled sentences\n",
            "./__MACOSX/sentiment labelled sentences/._.DS_Store\n",
            "./__MACOSX/sentiment labelled sentences/._imdb_labelled.txt\n",
            "./__MACOSX/sentiment labelled sentences/._readme.txt\n",
            "./__MACOSX/._sentiment labelled sentences\n",
            "./amazon_reviews.db\n",
            "./update.py\n",
            "./vectorizer.py\n",
            "./amazon_app.py\n",
            "./Copy of Project_02_Duncan_Ferguson_v2.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing the Directory before making the pickle\n",
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcz3okG4zf6g",
        "outputId": "cbfa1439-9222-43d7-f5ab-bb403c880f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Project_02_Duncan_Ferguson_v2.ipynb',\n",
              " '__pycache__',\n",
              " 'pkl_objects',\n",
              " 'static',\n",
              " 'templates',\n",
              " 'sentiment labelled sentences.zip',\n",
              " 'sentiment labelled sentences',\n",
              " '__MACOSX',\n",
              " 'amazon_reviews.db',\n",
              " 'update.py',\n",
              " 'vectorizer.py',\n",
              " 'amazon_app.py',\n",
              " 'Copy of Project_02_Duncan_Ferguson_v2.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pkl_objects/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg0K79jLFLZ1",
        "outputId": "5ec939e3-101c-4728-c056-d18d4f3dc9aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/content/MyDrive/ML_Colab/Project_2/pkl_objects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Making my two pickles... Stop words was not chosen by the hyper parama\n",
        "pickle.dump(stop, open('stopwords.pkl', 'wb'), protocol=4)\n",
        "pickle.dump(clf, open('classifier.pkl', 'wb'), protocol=4)"
      ],
      "metadata": {
        "id": "ceKf_Emo1qDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing the directory after making the pickle\n",
        "# Displaying that the pickles have been made\n",
        "display(os.getcwd())\n",
        "display(os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "aYKvC6bV_B-w",
        "outputId": "b800f570-1d3f-4eb7-c027-43d45a2d151a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'/content/content/MyDrive/ML_Colab/Project_2/pkl_objects'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['classifier.pkl', 'stopwords.pkl']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZHQmiMAFtPa",
        "outputId": "993e905d-b58d-4268-a3dd-f9b7a41711ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/content/MyDrive/ML_Colab/Project_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creat a standalone Python file"
      ],
      "metadata": {
        "id": "S9MfIXg05NwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective: File should include all of the data preprocessing functionality created in step 4 named vectorizer.py. \n",
        "### Objective: The creation of vectorizer.py should be done inside project02.pynb file similiar to ch09.ipynb"
      ],
      "metadata": {
        "id": "yvCbjMJy5TtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying that a vectorizer.py file has been created in the file\n",
        "display(os.getcwd())\n",
        "display(os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "EWIiU6Kq_5YK",
        "outputId": "91ecb92c-09e7-47e8-adb2-47d1ccee1a77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'/content/content/MyDrive/ML_Colab/Project_2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['Project_02_Duncan_Ferguson_v2.ipynb',\n",
              " '__pycache__',\n",
              " 'pkl_objects',\n",
              " 'static',\n",
              " 'templates',\n",
              " 'sentiment labelled sentences.zip',\n",
              " 'sentiment labelled sentences',\n",
              " '__MACOSX',\n",
              " 'amazon_reviews.db',\n",
              " 'update.py',\n",
              " 'vectorizer.py',\n",
              " 'amazon_app.py',\n",
              " 'Copy of Project_02_Duncan_Ferguson_v2.ipynb']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the other necessary folders\n",
        "- Making static folder\n",
        "- Making templates folder\n"
      ],
      "metadata": {
        "id": "on5lZykEF5R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories already made\n",
        "# %mkdir static\n",
        "# %mkdir templates"
      ],
      "metadata": {
        "id": "axg290PGGGgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(os.getcwd())\n",
        "display(os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "CyiUgmTQGPAM",
        "outputId": "de55691e-4624-4b69-a0af-81e5780ab123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'/content/content/MyDrive/ML_Colab/Project_2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['Project_02_Duncan_Ferguson_v2.ipynb',\n",
              " '__pycache__',\n",
              " 'pkl_objects',\n",
              " 'static',\n",
              " 'templates',\n",
              " 'sentiment labelled sentences.zip',\n",
              " 'sentiment labelled sentences',\n",
              " '__MACOSX',\n",
              " 'amazon_reviews.db',\n",
              " 'update.py',\n",
              " 'vectorizer.py',\n",
              " 'amazon_app.py',\n",
              " 'Copy of Project_02_Duncan_Ferguson_v2.ipynb']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective: This file will initialize a HashingVectorizer using the parameters I learned tuning TfidVectorizer in step Four\n",
        "- When you look in the file, you will find the same as created from the hashing vectorizer above."
      ],
      "metadata": {
        "id": "JmErxt99_5hv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Website Creation and Publishing"
      ],
      "metadata": {
        "id": "_MC308fh5q2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective: Create an interactive website using Flask, SQlite, WTforms, and Jinja2, that will enable me to test the accuracy of my calculated model by prompting users to enter text in the same form as your train data (product reviews, social media posts, etc. ) and perform sentiment analysis against their submissionts\n",
        "\n",
        "- Link to the working! Webapp: http://duncanferguson.pythonanywhere.com/"
      ],
      "metadata": {
        "id": "g3lWEFJY5ul8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Etb9dpeqPvJi",
        "outputId": "0e02128d-9507-42d7-8f4b-1416d2904e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/content/MyDrive/ML_Colab/Project_2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg2SRb0kQFkv",
        "outputId": "81969747-f1c0-449d-a218-a5010b9b2a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Project_02_Duncan_Ferguson_v2.ipynb',\n",
              " '__pycache__',\n",
              " 'pkl_objects',\n",
              " 'static',\n",
              " 'templates',\n",
              " 'sentiment labelled sentences.zip',\n",
              " 'sentiment labelled sentences',\n",
              " '__MACOSX',\n",
              " 'amazon_reviews.db',\n",
              " 'update.py',\n",
              " 'vectorizer.py',\n",
              " 'amazon_app.py',\n",
              " 'Copy of Project_02_Duncan_Ferguson_v2.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Objective: Very similar to Lecture 12 -Embedding a Machine Learning Model into a Web Application, I will then show the user my sentiment prediction base on their entry, allowing them to rate whether the prediction was accurate or not. \n",
        "\n",
        "- Completed: http://duncanferguson.pythonanywhere.com/\n",
        "\n"
      ],
      "metadata": {
        "id": "fOfRcDRiPvuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective: User provided examples will be stored in SQLite and be used to incrementally refine the trained model I created in step 4. \n",
        "\n"
      ],
      "metadata": {
        "id": "efKINc57PwMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('amazon_reviews.db')\n",
        "c = conn.cursor()\n",
        "c.execute('DROP TABLE IF EXISTS amazon_review_db')\n",
        "c.execute('CREATE TABLE amazon_review_db (review TEXT, sentiment INTEGER)')\n",
        "\n",
        "example1 = 'This product is horrible!'\n",
        "c.execute(\"INSERT INTO amazon_review_db (review, sentiment) VALUES (?, ?)\", (example1,0))\n",
        "\n",
        "example2 = 'Testing 1, Testing 2 Testing, This product should do the trick'\n",
        "c.execute(\"INSERT INTO amazon_review_db (review, sentiment) VALUES (?, ?)\", (example2,1))\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "N7wtJM5FPwSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect('amazon_reviews.db')\n",
        "c = conn.cursor()\n",
        "c.execute('Select * FROM amazon_review_db')\n",
        "results = c.fetchall()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "pTIXiibfo2K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi3bu7EYo3Nn",
        "outputId": "cf3a0841-d566-4494-c021-05b361300d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This product is horrible!', 0), ('Testing 1, Testing 2 Testing, This product should do the trick', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective: Complete website will be a published to PthonAnywhere and a link to the website will be included in a markdown cell in this Jupter notebook.\n",
        "\n",
        "- Completed: http://duncanferguson.pythonanywhere.com/\n",
        "\n"
      ],
      "metadata": {
        "id": "RWJEbBAVPwbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective: The source code for the website (including Python source code, HTML templates, and Python pickle files) will be submitted alongside the Jupyter Notbook in Goodle Drive \n",
        "\n"
      ],
      "metadata": {
        "id": "T5D584fXP3Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "2zGLoURsP52u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b755a9d3-aa96-4dae-f77d-1b254a760a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Project_02_Duncan_Ferguson_v2.ipynb',\n",
              " '__pycache__',\n",
              " 'pkl_objects',\n",
              " 'static',\n",
              " 'templates',\n",
              " 'sentiment labelled sentences.zip',\n",
              " 'sentiment labelled sentences',\n",
              " '__MACOSX',\n",
              " 'amazon_reviews.db',\n",
              " 'update.py',\n",
              " 'vectorizer.py',\n",
              " 'amazon_app.py',\n",
              " 'Copy of Project_02_Duncan_Ferguson_v2.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective Note: I must use the pickled model created in step four as the starting point for my classifier. In addition, if your webserver was to be restared, all user-provided examples must still represet in the trained model (i.e. I cannot rest the pickled version only on server restart)\n",
        "\n",
        "- The pickles from above were used. They were done in such a way that if the webserver was restated all user-provided examples would still be represent in the trained model.\n"
      ],
      "metadata": {
        "id": "bip3dMYgP6Ec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Objective Note: Make sure to look closely at the code thats being reused for the Flask application to make sure that assumptions built into the existing app still hold for the dataset. For instance, make sure that the handling of correct/incorrect feedback via the website works for my data.\n",
        "\n",
        "- Completed. Reused flask application is work and they are handling the correct/incorrect feedback properly.\n",
        "\n",
        "http://duncanferguson.pythonanywhere.com/"
      ],
      "metadata": {
        "id": "2biQZ6tuP7gt"
      }
    }
  ]
}