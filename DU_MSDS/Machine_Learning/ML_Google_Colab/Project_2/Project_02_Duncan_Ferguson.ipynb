{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project_02_Duncan_Ferguson.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN1ra6vVVItJ6kxXzv3HOKG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Objectives\n","- 1). Read in a classifiable training datset from the Internet using Python for sentiment analysis\n","\n","- 2). Understand the ethical implications of the dataset I've just identified, both in terms of the legality of its use, and in regards to any inherent biases present in the training data.\n","\n","- 3). Preprocessing the training dataset to clean unwanted text data, and extract n-grams from the eample documents that can be used for model training.\n","\n","- 4). Train a logistic regresion model using the cleaned training data that can be used to predict the sentiment of the unclassified examples\n","\n","- 5). Use a grid search to optimize the logistic regression model.\n","\n","- 6). Deploy the optimized and trained logistic regression model to the web using Python pickling, and provide a web form for users to further incrementally train your model by entering additional sample data and providing feedback about its classification accuracy."],"metadata":{"id":"505kQxu_1ONK"}},{"cell_type":"markdown","source":["# Experiment Objective"],"metadata":{"id":"uma7XtwV2PE1"}},{"cell_type":"markdown","source":["- Describe the dataset I am analyzing, including the TOS and ethical considerations discussed in Lecture 07 & 08 \n","\n","- Why did I choose the training examples I did for training my model?\n","\n","- How is sentiment determined for the training data?\n"],"metadata":{"id":"6KNVGaFh2SIw"}},{"cell_type":"markdown","source":["# Data Collection"],"metadata":{"id":"UE7CkbxZ2mCt"}},{"cell_type":"markdown","source":["- Write the necessary Python code to retrieve and store the training examples to create the model.\n","\n","- Am I accesing an API? Scaping a website? Downloading from an archive?\n","\n","- The resulting implementation should be an in-memory pandas dataframe"],"metadata":{"id":"zxaImevr2oFK"}},{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"C94Nxv7K25O9"}},{"cell_type":"markdown","source":["- This is where the preprocessing utility for the project will be built.\n","\n","- Steps required, clean text data, tokenize the document, construct a TfidfVectorizer to be performed here"],"metadata":{"id":"2xSohlCh267V"}},{"cell_type":"markdown","source":["# Model Optimization and Serialization"],"metadata":{"id":"r4Gzadgw3WkV"}},{"cell_type":"markdown","source":["- This is where I will use a grid search similar to the one performed in lecture 11: Applying Machine Learning to Sentiment Anlysis\n","\n","- Find the optimal hyperparameters (including choice of stemming algorithm for TfidVectorizer) to use with my out-of-core capable logistic regression classifier (like SGDClassifier)  Utilizing the TfidVectorizerI created in tepp three to preprocess my data\n","\n","- Document the results of the grid seach in a marckdown cell immediately after the grid search, including computed accuracies agianst my training and test data sets.\n","\n","- After documenting the finding fit the classifier against my entire dataset, then pickle the resulting object to a file that will be used in the website\n","\n","* Note: Out-of-core learning is a requirement. Since I will be updating the model using the deployued Flask application, the classifier I pickle for yuse on the website must support incremental learning\n","\n","* Note the hyperparamters I explore in the grid search must include different values than we used in class. One of the goals of this project is to see if other options migh yield better results than the book attempted. \n","\n","- Be sure to exaplain in the write up for this section why I chose the values that I did for the hyperparameters, and if they performed better for my data set than the other ones the booke attempted to use\n","\n","* NOte: The grid search I perform will be agianst an something like an SGDClassifier instance, instead of the LogisticRegrssion classifier that was used in the book if you use SFDClassifer, instead of varying the value of C for regularization, you will vary the values for alpha. Still compare l1 vs l2 regulatization. This will result in a comparison of my work to the example in the book that isnt' strictly 1-1 but will be close enough to draw meaningful conculsions against\n"],"metadata":{"id":"VJcWoh1J3a8h"}},{"cell_type":"markdown","source":["# Creat a standalone Python file"],"metadata":{"id":"S9MfIXg05NwR"}},{"cell_type":"markdown","source":["- File shoudl include all of the data preprocessing functionality created in step 4 named vectorizer.py. This file will initialize a HashingVectorizer using the parameters I learned tuning TfidVectorizer in step Four\n","- The creation of vectorizer.py should be done inside project02,pynb file similiar to ch09.ipynb"],"metadata":{"id":"yvCbjMJy5TtN"}},{"cell_type":"markdown","source":["# Website Creation and Publishing"],"metadata":{"id":"_MC308fh5q2_"}},{"cell_type":"markdown","source":["- Create an interactive website using Flask, SQlite, WTforms, and Jinja2, that will enable me to test the accuracy of my calculated model by prompting users to enter text in the same form as your train data (product reviews, social media posts, etc. ) and perform sentiment analysis against their submissionts\n","\n","- Very similar to Lecture 12 -Embedding a Machine Learning Model into a Web Application, I will then show the user my sentiment prediction base on their entry, allowing them to rate whether the prediction was accurate or not. \n","\n","- User provided examples will be stored in SQLite and be used to incrementally refine the trained model I created in step 4. \n","\n","- Complete website will be a published to PthonAnywhere and a link to the website will be included in a markdown cell in this Jupter notebook.\n","\n","- The source code for the website (including Python source code, HTML templates, and Python pickle files) will be submitted alongside the Jupyter Notbook in Goodle Drive \n","\n","- Note: I must use the pickled model created in step four as the starting point for my classifier. In addition, if your webserver was to be restared, all user-provided examples must still represet in the trained model (i.e. I cannot rest the pickled version only on server restart)\n","\n","- Note: Make sure to look closely at the code thats being reused for the Flask application to make sure that assumptions built into the existing app still hold for the dataset. For instance, make sure that the handling of correct/incorrect feedback via the website works for my data."],"metadata":{"id":"g3lWEFJY5ul8"}}]}