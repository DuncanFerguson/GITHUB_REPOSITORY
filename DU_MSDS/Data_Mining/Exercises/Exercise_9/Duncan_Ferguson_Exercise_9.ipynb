{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Student: Duncan Ferguson <br>\n",
    "Student Id: 871641260 <br>\n",
    "Class: Comp 4431-1 <br>\n",
    "Assignment: Exercise 9 <br>\n",
    "Date: 11/5/2021 <br>\n",
    "Group: Name: Broken Toe <br>\n",
    "Group Members: Emma Bright, Mike Santoro <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from matplotlib import pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def equalProb():  # returns 0 or 1 with equal probablity\n",
    "\tif (np.random.random() < 0.95):\n",
    "\t\treturn(0)\n",
    "\telse:\n",
    "\t\treturn (1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def boundedNormal(bound,stddev):\n",
    "\trNum = np.random.normal(0, stddev)\n",
    "\twhile rNum < -bound or rNum > bound:\n",
    "\t\trNum = np.random.normal(0, stddev)\n",
    "\treturn (rNum)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def create_clusters1():\n",
    "    \"\"\"This Function Creates the Data Set for the First Clustering Visualization\"\"\"\n",
    "    center1 = 5\n",
    "    center2 = 15\n",
    "    center3 = 30\n",
    "    bound = 5\n",
    "    stddev = 2\n",
    "\n",
    "    tuples = []\n",
    "    numTuples = 1000\n",
    "\n",
    "    for i in range(numTuples):\n",
    "        rNum = np.random.randint(3)\n",
    "        if (rNum == 0):\n",
    "            candidate = 0\n",
    "            rNum = boundedNormal(bound,stddev)\n",
    "            a1 = center1 + rNum\n",
    "            rNum = boundedNormal(bound,stddev)\n",
    "            a2 = center1 + rNum\n",
    "        elif (rNum == 1):\n",
    "            candidate = 1\n",
    "            rNum = boundedNormal(bound+10,stddev+10)\n",
    "            a1 = center2 + rNum\n",
    "            rNum = boundedNormal(bound+5,stddev+5)\n",
    "            a2 = center2 + rNum + 5\n",
    "\n",
    "        else:\n",
    "            candidate = 2\n",
    "            rNum = boundedNormal(bound+4,stddev)\n",
    "            a1 = center3 + rNum\n",
    "            rNum = boundedNormal(bound+2,stddev)\n",
    "            a2 = center3 + rNum\n",
    "\n",
    "        df_cluster_1 = pd.DataFrame(tuples,columns=['a1','a2'])\n",
    "        return df_cluster_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def create_clusters2():\n",
    "    \"\"\"This Function Creates the Data set for visualizing Clusters for the second visualization\n",
    "    It is essentially a circle to help display how the different algorithms work on a circle\"\"\"\n",
    "    center1 = 15\n",
    "    center2 = 15\n",
    "    center3 = 15\n",
    "    bound = 25\n",
    "    stddev = 2\n",
    "\n",
    "    tuples = []\n",
    "    numTuples = 1000\n",
    "\n",
    "    for i in range(numTuples):\n",
    "        rNum = np.random.randint(3)\n",
    "        if (rNum == 0):\n",
    "            candidate = 0\n",
    "            rNum = boundedNormal(bound, stddev)\n",
    "            a1 = center1 + rNum\n",
    "            rNum = boundedNormal(bound, stddev)\n",
    "            a2 = center1 + rNum\n",
    "\n",
    "        elif (rNum == 1):\n",
    "            candidate = 1\n",
    "            rNum = boundedNormal(bound, stddev)\n",
    "            a1 = center2 + rNum\n",
    "            rNum = boundedNormal(bound, stddev)\n",
    "            a2 = center2 + rNum\n",
    "\n",
    "        else:\n",
    "            candidate = 2\n",
    "            rNum = boundedNormal(bound, stddev)\n",
    "            a1 = center3 + rNum\n",
    "            rNum = boundedNormal(bound, stddev)\n",
    "            a2 = center3 + rNum\n",
    "\n",
    "        atuple = (a1,a2)\n",
    "        tuples.append(atuple)\n",
    "\n",
    "    df_cluster_2 = pd.DataFrame(tuples,columns=['a1','a2'])\n",
    "    return df_cluster_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def plot_clustering(theData, kmeanLabels, brcLabels, aggLabels, dbLabels):\n",
    "\tfig,ax = plt.subplots(2, 2)\n",
    "\n",
    "\tfor i in range(len(theData)):\n",
    "\t\tif kmeanLabels[i] == 0: theColor = 'red'\n",
    "\t\tif kmeanLabels[i] == 1: theColor = 'green'\n",
    "\t\tif kmeanLabels[i] == 2: theColor = 'blue'\n",
    "\t\tax[0,0].scatter(theData[i][0], theData[i][1], s=9.5, alpha=1.0, color=theColor)\n",
    "\t\tax[0,0].set_title('kmeans')\n",
    "\n",
    "\tfor i in range(len(theData)):\n",
    "\t\tif brcLabels[i] == 0: theColor = 'blue'\n",
    "\t\tif brcLabels[i] == 1: theColor = 'red'\n",
    "\t\tif brcLabels[i] == 2: theColor = 'green'\n",
    "\t\tax[0,1].scatter(theData[i][0], theData[i][1], s=9.5, alpha=1.0, color=theColor)\n",
    "\t\tax[0,1].set_title('birch')\n",
    "\n",
    "\tfor i in range(len(theData)):\n",
    "\t\tif aggLabels[i] == 0: theColor = 'red'\n",
    "\t\tif aggLabels[i] == 1: theColor = 'green'\n",
    "\t\tif aggLabels[i] == 2: theColor = 'blue'\n",
    "\t\tax[1,0].scatter(theData[i][0], theData[i][1], s=9.5, alpha=1.0, color=theColor)\n",
    "\t\tax[1,0].set_title('agglomerative')\n",
    "\n",
    "\tfor i in range(len(theData)):\n",
    "\t\tif dbLabels[i] == 0: theColor = 'red'\n",
    "\t\telif dbLabels[i] == 1: theColor = 'green'\n",
    "\t\telif dbLabels[i] == 2: theColor = 'blue'\n",
    "\t\telse: theColor = 'pink'\n",
    "\t\tax[1,1].scatter(theData[i][0], theData[i][1], s=9.5, alpha=1.0, color=theColor)\n",
    "\t\tax[1,1].set_title('dbscan')\n",
    "\n",
    "\tplt.figure(figsize=(7, 5))\n",
    "\tplt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def run_main(df):\n",
    "    \"\"\"This Function Runs the code for the given data set\"\"\"\n",
    "    numClusters = 3\n",
    "    print(df)\n",
    "    theData = df.to_numpy()\n",
    "    print(df)\n",
    "    # ########## kmeans\n",
    "    # kmeans = KMeans(n_clusters=numClusters, random_state=0).fit(df)\n",
    "    #\n",
    "    # ########## birch\n",
    "    # brc = Birch(n_clusters=numClusters).fit(df)\n",
    "    # clusteredData = dict()\n",
    "    # for i in range(numClusters):\n",
    "    #     clusteredData[i] = list()\n",
    "    # for i in theData:\n",
    "    #     clusterNum = brc.predict([i])[0]\n",
    "    #     clusteredData[clusterNum].append(i)\n",
    "    #\n",
    "    # ########## Agglomerative\n",
    "    # agg = AgglomerativeClustering(n_clusters=numClusters,linkage='ward').fit(df)\n",
    "    #\n",
    "    # ########## DBSCAN\n",
    "    # db = DBSCAN(eps=1.5, min_samples=4).fit(df)\n",
    "    # core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    # core_samples_mask[db.core_sample_indices_] = True\n",
    "    #\n",
    "    # ###### compare them\n",
    "    # theData = df.to_numpy()\n",
    "    # plot_clustering(theData, kmeans.labels_ , brc.labels_, agg.labels_, db.labels_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [a1, a2]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('outfile_Square.csv')\n",
    "np.random.seed(1)\n",
    "df = create_clusters1()\n",
    "print(df)\n",
    "\n",
    "# run_main(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running through the descriptions on Cluster set 1\n",
    "\"\"\"\n",
    "A pdf file containing a writeup describing one or two data sets, the 4-char visualization that shows\n",
    "the results in different clustering, and an explanation of WHY the different clustering algorithms behave\n",
    "as they do\"\"\"\n",
    "\n",
    "### K-means\n",
    "K- Means starts by clustering the data into n amount of clusters. A random n centroids are selected\n",
    "and then the data is parsed through and each point is then assigned to the cluster in which the centroid it\n",
    "is closest to. The mean of the n clusters is then calculated and theses are assigned as the new centroids.\n",
    "The algrorithm then repeats this processes until the centroids stabalize.\n",
    "\n",
    "The reason that we have outliers in figure the dispersion occurs because the\n",
    "\n",
    "### Birch\n",
    "https://www.geeksforgeeks.org/ml-birch-clustering/\n",
    "Birch starts by building a Clustering Feature Tree. Theses features branches are then broken down into\n",
    "smaller CF tree's. These smaller trees are then Clustered together for the desired amount of clusters.\n",
    "\n",
    "A CF tree is a tree where each leaf node contains a sub-cluster. Every entry in a CF tree contains a pointer to a\n",
    "child node and a CF entry made up of the sum of CF entries in the child nodes. There is a maximum number of entries\n",
    "in each leaf node. This maximum number is called the threshold\n",
    "\n",
    "In figure 1 for the birch algorithm the decision tree breaks the data into\n",
    "-It tries to minimize the distance between any of the points in the trees.\n",
    "-It's going to try to group everything to the closest point\n",
    "-Uses linear sum and the quadratic sums minimize\n",
    "-Average distance from the centroid to any point in the cluster\n",
    "also the average distance between two points.\n",
    "It tries to aoid those long cigar type shapes. Because then the distances get long\n",
    "If it is in a nice ball the average distance get smaller.\n",
    "\n",
    "\n",
    "The difference in figure one the birch algorithm looks more square because it is trying to minimize the\n",
    "distance between all of the points from eachother. This makes the data look more square like than the K means.\n",
    "\n",
    "### Agglomerative\n",
    "https://www.geeksforgeeks.org/agglomerative-methods-in-machine-learning/\n",
    "Starts with everything in their own cluster\n",
    "- Any Pair Wise Distance\n",
    "- Use's a bottom up approach\n",
    "- 'Ward' linkage,\n",
    "-Uses the bottom-up approach. It starts with each object forming its own cluster\n",
    "- then iteratively merges the clusters according to their similarity to form large cluster\n",
    "\n",
    "-In Figure 1. The Red Cluster starts to have a larger variance as it joins clusters together\n",
    "This then increases its variance an and add a larger set of data. Where as the two smaller clusters\n",
    "In Green and Blue pull a tighter spread together.\n",
    "\n",
    "-Agglomerative uses a similiar to the k-means in the way in which the algorithm works\n",
    "which is why they look the same.\n",
    "\n",
    "### dbscan\n",
    "https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/\n",
    "Assumes the data is spherical. It will calculate sep\n",
    "Differences between nearest points\n",
    "-Is based on the intuitive notion of \"clusters\" and noise. The key idea is that for\n",
    "each point of a cluster, the neighborhood of a given radius has to contain atleast a minumum\n",
    "number of points\n",
    "-DB scan helps create clusters that have arbitrary shapes that might not be spheres.\n",
    "-DB scan works well when the data may have some noise\n",
    "\n",
    "-DBSCAN requires two parameters:\n",
    "1). EPS It defines the neighborhood around a data point. If the distances between two points is lower\n",
    "or equal to the eps then they are considered as neighbors. If the eps value chosen is too small\n",
    "then large part of the data will be considered outlires.\n",
    "If the EPS is large then the clusters will merge together.\n",
    "EPS k-distrance Graph\n",
    "\n",
    "\n",
    "2). MinPts. Minimum number of neighbors (data points) within eps radius.\n",
    "The larger the data set the larger the minpoints must be chosen.\n",
    "The min points is the amount of dimensions +1\n",
    "\n",
    "Minpoints must be three"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# df2 = create_clusters2()\n",
    "# run_main(df2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-865e1948",
   "language": "python",
   "display_name": "PyCharm (GITHUB_REPOSITORY)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}