{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 10\n",
    "\n",
    "Date: 11/16/2021 <br>\n",
    "Group Name: Broken Toes <br>\n",
    "Group Members: Emma Bright, Mike Santoro, Duncan Ferguson\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1:  PCA\n",
    "\n",
    "Principle Component Analysis (PCA) can be used to reduce the time needed to build machine learning models.  You are to apply PCA the weather prediction data set using GausianNP.\n",
    "First create a table (or chart) of the explained_variance_ratio_ for the data set.  Then run your models for 1, 2, 3, 4, 5, 6, 7, 8, and all components.  Create a table (or chart) showing the accuracy as a function of the number of  components."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a function to perform the Gaussian NB machine learning algorithm."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def runGaussianNB(x_train, x_test, y_train, y_test):\n",
    "\tglobal gausianNB_predicted\n",
    "\tmodel = GaussianNB()\n",
    "\tmodel.fit(x_train,y_train)\n",
    "\tgausianNB_predicted = model.predict(x_test)\n",
    "\taccuracy = accuracy_score(y_test, gausianNB_predicted)\n",
    "\timps = permutation_importance(model, x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read in the weather data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleanInfile.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a table of the Explained Variance Ratio for the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "Y = df['RainTomorrow']\n",
    "df2 = df.drop('RainTomorrow',axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test  = train_test_split( df2, Y,  test_size=0.20, random_state=1)\n",
    "save_xtrain = x_train\n",
    "save_xtest = x_test\n",
    "save_ytrain = y_train\n",
    "save_ytest = y_test\n",
    "runGaussianNB(x_train, x_test, y_train, y_test)\n",
    "\n",
    "pca = PCA()\n",
    "x_train = pca.fit_transform(save_xtrain)\n",
    "x_test = pca.fit_transform(save_xtest)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"explained_variance = \")\n",
    "print(explained_variance)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the Gaussian NB model for 1, 2, 3, 4, 5, 6, 7, 8, and all components."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy_list= []\n",
    "x_train = save_xtrain\n",
    "x_test = save_xtest\n",
    "# Run with all components\n",
    "runGaussianNB(x_train, x_test, y_train, y_test)\n",
    "accuracy_list.append(['all', accuracy_score(y_test, gausianNB_predicted)])\n",
    "\n",
    "\n",
    "# Run with 1, 2, 3....., 8 components\n",
    "for i in range (1, 9):\n",
    "    pca = PCA(n_components=i)\n",
    "    x_train = pca.fit_transform(save_xtrain)\n",
    "    x_test = pca.fit_transform(save_xtest)\n",
    "    runGaussianNB(x_train, x_test, y_train, y_test)\n",
    "    accuracy_list.append( [str(i), accuracy_score(y_test, gausianNB_predicted)])\n",
    "\n",
    "result = pd.DataFrame(accuracy_list, columns=[\"Number of Components\", \"Accuracy\"])\n",
    "result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Based on this, how many components do you recommend using for constructing the GausianNB model for this data set?\n",
    "\n",
    "Based on this output I would recommend using 4 components for the Gaussian Naive Bayes model as this has the highest level of accuracy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "\n",
    "### Part 2: Clustering Quality\n",
    "\n",
    "Apply kmeans and dbmeans clustering to these labeled data sets - outfile1, outfile2, and outfile3.\n",
    "Calculate the following three metrics on the clustering for kmeans and dbscan:  homogeneity, completeness, and adjusted_mutual_info_score."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numClusters = 3\n",
    "\n",
    "outfile = pd.read_csv('outfile.csv')\n",
    "labels1 = pd.read_csv('outfile1.csv')\n",
    "labels2 = pd.read_csv('outfile2.csv')\n",
    "labels3 = pd.read_csv('outfile3.csv')\n",
    "\n",
    "kmeans = KMeans(n_clusters=numClusters, random_state=0).fit(outfile)\n",
    "db = DBSCAN(eps=1.5, min_samples=4).fit(outfile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outfile['predKM'] = kmeans.labels_\n",
    "outfile['predDB'] = db.labels_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outfile.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels1['actual'] = 0\n",
    "labels2['actual'] = 1\n",
    "labels3['actual'] = 2\n",
    "labels = labels1.append(labels2, ignore_index = True)\n",
    "labels = labels.append(labels3, ignore_index = True)\n",
    "labels.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.merge(labels, outfile,  how='left', left_on=['a1','a2'], right_on = ['a1','a2'])\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('K-Means')\n",
    "print(f\"Homogenity Score: {homogeneity_score(df.actual.to_list(),df.predKM.to_list())}\")\n",
    "print(f'Completeness Score: {completeness_score(df.actual.to_list(), df.predKM.to_list())}')\n",
    "print(f'Adjusted Mutual Info Score: {adjusted_mutual_info_score(df.actual.to_list(), df.predKM.to_list())}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('DB Scan')\n",
    "print(f\"Homogenity Score: {homogeneity_score(df.actual.to_list(),df.predDB.to_list())}\")\n",
    "print(f'Completeness Score: {completeness_score(df.actual.to_list(), df.predDB.to_list())}')\n",
    "print(f'Adjusted Mutual Info Score: {adjusted_mutual_info_score(df.actual.to_list(), df.predDB.to_list())}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### A description of what these metrics tell you about the clustering?\n",
    "\n",
    "Homogeneity Score is the measure that each cluster contains only members of a single class. <br>\n",
    "The Completeness Score is a measure that all members of a given class are assigned to the same cluster. The two previous metrics can be combined to obtain the Adjusted Mutual Info Score since both are bounded below by 0.0 and above by 1.0 (higher is better).<br>\n",
    "We can see from that excercise that DB-scan was much more effective at clustering this data-set as all of the scores for the metrics are much closer to 1 than those of K-Means."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}