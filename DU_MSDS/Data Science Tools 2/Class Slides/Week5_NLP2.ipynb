{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text normalization cont.\n",
    "Last time we finished with some text normalization activities like stemming and normalization (removing [inflectional](https://en.wikipedia.org/wiki/Inflection) affixes**(ed, ing, ize, s, de)**).\n",
    "\n",
    "Note that \n",
    "\n",
    "- stemming can result in a word not in the dictionary.\n",
    "- Lemmatization ensures word is in dictionary.\n",
    "- stemming is a fast process compared to lemmatization.\n",
    "\n",
    "\n",
    "We can use use both the techniques to further reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/csresearch/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "word= 'muses'\n",
    "ps.stem(word)\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mus\n",
      "mu\n"
     ]
    }
   ],
   "source": [
    "wn_lm= WordNetLemmatizer()\n",
    "lm_word = wn_lm.lemmatize(word)\n",
    "print(lm_word)\n",
    "print(ps.stem(lm_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expanding contractions\n",
    "\n",
    "This activity invloves replacing contractions with full words like\n",
    "- can't with cannot.\n",
    "- Should've with should have\n",
    "- Weren't were not\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Any suggestion how to do it? What module and function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "contraction_patterns=[(r'can\\'t', 'cannot'),\n",
    "                    (r'haven\\'t', 'have not'),\n",
    "                    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "                    (r'(\\w+)\\'re', '\\g<1> are')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class contraction_replacer(object):\n",
    "    def __init__(self, contraction_patterns):        \n",
    "        # store compiled regex object\n",
    "        self._contraction_regexes = [(re.compile(p), replaced_text) for p, replaced_text in contraction_patterns]\n",
    "        \n",
    "    def do_contraction_normalization(self, text):\n",
    "        for contraction_regex, replaced_text in self._contraction_regexes:\n",
    "            text = contraction_regex.sub(replaced_text, text)\n",
    "        return text     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Let's use it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample_contraction_replacer = contraction_replacer(contraction_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We will make this work'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_contraction_replacer.do_contraction_normalization(\"We'll make this work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'will', 'make', 'this', 'work']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing contraction and tokenize\n",
    "nltk.tokenize.word_tokenize(sample_contraction_replacer.do_contraction_normalization(\"We'll make this work\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Removing repeated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class repeat_replacer(object):\n",
    "    def __init__(self, repeat_patterns, sub_pattern):       \n",
    "        \n",
    "        # store compiled regex object\n",
    "        self._repeat_regexes = re.compile(repeat_patterns)\n",
    "        self._sub_pattern = sub_pattern\n",
    "    def do_repeat_normalization(self, word):\n",
    "        compressed_word = self._repeat_regexes.sub(self._sub_pattern, word)\n",
    "        if compressed_word != word:\n",
    "            compressed_word = self.do_repeat_normalization(compressed_word)\n",
    "            \n",
    "        \n",
    "        return compressed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Notice how backreferences(\\1, \\2, \\3) are used  I looove loove love \n",
    "sample_repeat_replacer = repeat_replacer(r'(\\w*)(\\w)\\2(\\w*)', r'\\1\\2\\3' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('oh', 'love')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_repeat_replacer.do_repeat_normalization('ooooh'), sample_repeat_replacer.do_repeat_normalization('loooove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What happens when word has repeating character!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shep'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_repeat_replacer.do_repeat_normalization('sheep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "class repeat_replacer(object):\n",
    "    def __init__(self, repeat_patterns, sub_pattern):\n",
    "        # store compiled regex object\n",
    "        self._repeat_regexes = re.compile(repeat_patterns)\n",
    "        self._sub_pattern = sub_pattern\n",
    "    def do_repeat_normalization(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        compressed_word = self._repeat_regexes.sub(self._sub_pattern, word)\n",
    "        if compressed_word != word:\n",
    "            #print('iside if')\n",
    "            compressed_word = self.do_repeat_normalization(compressed_word)\n",
    "        return compressed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "repeat_replacer_inst = repeat_replacer(r'(\\w*)(\\w)\\2(\\w*)', r'\\1\\2\\3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sheep'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat_replacer_inst.do_repeat_normalization('sheep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spelling correction with Enchant\n",
    "\n",
    "Go to \n",
    "http://www.abisource.com/projects/enchant/ \n",
    "to learn more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#You may get an error about the C libraries. To solve that issue, use HomeBrew (quick Google search)\n",
    "# brew update\n",
    "# brew install enchant\n",
    "\n",
    "\n",
    "# !pip install pyenchant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# To build a spell checker class we need\n",
    "- a spellchecking library like enchant. We just installed it\n",
    "- and a dictionary for it to use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let' see how enchant works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sk_SK', <Enchant: Aspell Provider>),\n",
       " ('mk', <Enchant: Aspell Provider>),\n",
       " ('nds', <Enchant: Aspell Provider>),\n",
       " ('tk', <Enchant: Aspell Provider>),\n",
       " ('be', <Enchant: Aspell Provider>),\n",
       " ('zu', <Enchant: Aspell Provider>),\n",
       " ('nl_NL', <Enchant: AppleSpell Provider>),\n",
       " ('ny', <Enchant: Aspell Provider>),\n",
       " ('ml', <Enchant: Aspell Provider>),\n",
       " ('lv', <Enchant: Aspell Provider>),\n",
       " ('sv', <Enchant: Aspell Provider>),\n",
       " ('cs', <Enchant: Aspell Provider>),\n",
       " ('de_DE', <Enchant: Aspell Provider>),\n",
       " ('fa', <Enchant: Aspell Provider>),\n",
       " ('pt_BR', <Enchant: Aspell Provider>),\n",
       " ('nn', <Enchant: Aspell Provider>),\n",
       " ('ku', <Enchant: Aspell Provider>),\n",
       " ('fr_CH', <Enchant: Aspell Provider>),\n",
       " ('br', <Enchant: Aspell Provider>),\n",
       " ('sk', <Enchant: Aspell Provider>),\n",
       " ('ru', <Enchant: Aspell Provider>),\n",
       " ('he', <Enchant: Aspell Provider>),\n",
       " ('sw', <Enchant: Aspell Provider>),\n",
       " ('fy', <Enchant: Aspell Provider>),\n",
       " ('bg', <Enchant: Aspell Provider>),\n",
       " ('el', <Enchant: Aspell Provider>),\n",
       " ('sl', <Enchant: Aspell Provider>),\n",
       " ('be_SU', <Enchant: Aspell Provider>),\n",
       " ('gd', <Enchant: Aspell Provider>),\n",
       " ('mn', <Enchant: Aspell Provider>),\n",
       " ('af', <Enchant: Aspell Provider>),\n",
       " ('la', <Enchant: Aspell Provider>),\n",
       " ('uz', <Enchant: Aspell Provider>),\n",
       " ('tn', <Enchant: Aspell Provider>),\n",
       " ('qu', <Enchant: Aspell Provider>),\n",
       " ('ar', <Enchant: Aspell Provider>),\n",
       " ('hr', <Enchant: Aspell Provider>),\n",
       " ('or', <Enchant: Aspell Provider>),\n",
       " ('pt_PT', <Enchant: Aspell Provider>),\n",
       " ('fo', <Enchant: Aspell Provider>),\n",
       " ('it', <Enchant: Aspell Provider>),\n",
       " ('rw', <Enchant: Aspell Provider>),\n",
       " ('en_CA', <Enchant: Aspell Provider>),\n",
       " ('en_US', <Enchant: Aspell Provider>),\n",
       " ('en_AU', <Enchant: Aspell Provider>),\n",
       " ('grc', <Enchant: Aspell Provider>),\n",
       " ('en', <Enchant: Aspell Provider>),\n",
       " ('da', <Enchant: Aspell Provider>),\n",
       " ('ta', <Enchant: Aspell Provider>),\n",
       " ('srd', <Enchant: Aspell Provider>),\n",
       " ('tl', <Enchant: Aspell Provider>),\n",
       " ('gr', <Enchant: Aspell Provider>),\n",
       " ('sc', <Enchant: Aspell Provider>),\n",
       " ('eo', <Enchant: Aspell Provider>),\n",
       " ('hsb', <Enchant: Aspell Provider>),\n",
       " ('ky', <Enchant: Aspell Provider>),\n",
       " ('te', <Enchant: Aspell Provider>),\n",
       " ('de_CH', <Enchant: Aspell Provider>),\n",
       " ('hi', <Enchant: Aspell Provider>),\n",
       " ('kn', <Enchant: Aspell Provider>),\n",
       " ('ca', <Enchant: Aspell Provider>),\n",
       " ('vi', <Enchant: Aspell Provider>),\n",
       " ('it_IT', <Enchant: AppleSpell Provider>),\n",
       " ('hu', <Enchant: Aspell Provider>),\n",
       " ('fr_FR', <Enchant: Aspell Provider>),\n",
       " ('fr', <Enchant: Aspell Provider>),\n",
       " ('mr', <Enchant: Aspell Provider>),\n",
       " ('tr', <Enchant: Aspell Provider>),\n",
       " ('cy', <Enchant: Aspell Provider>),\n",
       " ('ro', <Enchant: Aspell Provider>),\n",
       " ('yi', <Enchant: Aspell Provider>),\n",
       " ('mg', <Enchant: Aspell Provider>),\n",
       " ('pl', <Enchant: Aspell Provider>),\n",
       " ('hu_HU', <Enchant: AppleSpell Provider>),\n",
       " ('hus', <Enchant: Aspell Provider>),\n",
       " ('sv_SE', <Enchant: AppleSpell Provider>),\n",
       " ('ms', <Enchant: Aspell Provider>),\n",
       " ('ia', <Enchant: Aspell Provider>),\n",
       " ('pa', <Enchant: Aspell Provider>),\n",
       " ('gu', <Enchant: Aspell Provider>),\n",
       " ('wa', <Enchant: Aspell Provider>),\n",
       " ('de', <Enchant: Aspell Provider>),\n",
       " ('sr', <Enchant: Aspell Provider>),\n",
       " ('hil', <Enchant: Aspell Provider>),\n",
       " ('mt', <Enchant: Aspell Provider>),\n",
       " ('ast', <Enchant: Aspell Provider>),\n",
       " ('gv', <Enchant: Aspell Provider>),\n",
       " ('fi', <Enchant: Aspell Provider>),\n",
       " ('es', <Enchant: Aspell Provider>),\n",
       " ('mi', <Enchant: Aspell Provider>),\n",
       " ('de_AT', <Enchant: Aspell Provider>),\n",
       " ('bn', <Enchant: Aspell Provider>),\n",
       " ('csb', <Enchant: Aspell Provider>),\n",
       " ('en_GB', <Enchant: Aspell Provider>),\n",
       " ('uk', <Enchant: Aspell Provider>),\n",
       " ('am', <Enchant: Aspell Provider>),\n",
       " ('tet', <Enchant: Aspell Provider>),\n",
       " ('et', <Enchant: Aspell Provider>),\n",
       " ('lt', <Enchant: Aspell Provider>),\n",
       " ('hy', <Enchant: Aspell Provider>),\n",
       " ('gl', <Enchant: Aspell Provider>),\n",
       " ('nl', <Enchant: Aspell Provider>),\n",
       " ('es_ES', <Enchant: AppleSpell Provider>),\n",
       " ('id', <Enchant: Aspell Provider>),\n",
       " ('ga', <Enchant: Aspell Provider>),\n",
       " ('az', <Enchant: Aspell Provider>),\n",
       " ('be_BY', <Enchant: Aspell Provider>)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import enchant\n",
    "enchant.list_dicts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the English dictionary\n",
    "dict_int = enchant.Dict('en')\n",
    "\n",
    "#Check if the words 'love' and 'lov' exist in the dictionary\n",
    "dict_int.check('love'), dict_int.check('lov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scion',\n",
       " 'cine',\n",
       " 'sine',\n",
       " 'scene',\n",
       " 'Seine',\n",
       " 'Sen',\n",
       " 'seine',\n",
       " 'sen',\n",
       " 'sin',\n",
       " 'seen',\n",
       " 'sign',\n",
       " 'sci en',\n",
       " 'sci-en',\n",
       " 'scone',\n",
       " 'sienna',\n",
       " 'Sean',\n",
       " 'seeing',\n",
       " 'sewn',\n",
       " 'sing',\n",
       " 'sinew',\n",
       " 'scent',\n",
       " 'siren',\n",
       " 'Sn',\n",
       " 'sane',\n",
       " 'sicken',\n",
       " 'zine',\n",
       " 'Stein',\n",
       " 'Stine',\n",
       " 'sci',\n",
       " 'science',\n",
       " 'skein',\n",
       " 'spine',\n",
       " 'stein',\n",
       " 'swine',\n",
       " 'seiner']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the suggestions for the word 'scien'\n",
    "dict_int.suggest('scien')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How edit distance works\n",
    "\n",
    "*minimum number of character changes to transform one word into another.*\n",
    "\n",
    "See wiki for details\n",
    "https://en.wikipedia.org/wiki/Edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "\n",
    "#What is the distance between these two words ?\n",
    "edit_distance('sciena', 'science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let's write the class for performing spell correction\n",
    "- import enchant and initialize a dictionary(will use opensource aspell http://aspell.net/) for it to use\n",
    "- import edit_distance  from nltk.metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "import numpy as np\n",
    "\n",
    "class spell_checker(object):\n",
    "    def __init__(self, dict_name='en_US', max_edit_dist=3):\n",
    "        self._dict= enchant.Dict(dict_name)\n",
    "        self._max_edit_dist = max_edit_dist\n",
    "    def _word_with_min_dist(self, word, suggestions):\n",
    "        print(suggestions)\n",
    "        #min_edit_distance = np.inf\n",
    "        corrected_word = word\n",
    "        for sug in [suggestions[0]]:\n",
    "            distance = edit_distance(word, sug)\n",
    "            #print(distance)\n",
    "            if distance < self._max_edit_dist:\n",
    "                print(distance, sug)\n",
    "                min_edit_distance = distance\n",
    "                corrected_word = sug\n",
    "        return corrected_word        \n",
    "                \n",
    "                \n",
    "        \n",
    "    def check_spell(self, word):\n",
    "        if self._dict.check(word):\n",
    "            return word\n",
    "        # the the words with minimum distance\n",
    "        return self._word_with_min_dist(word, self._dict.suggest(word))\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "spell_check_int = spell_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jukebox', 'Quebec', 'cubic', 'jujube', 'kickback', 'jujubes', \"jujube's\", 'jojoba', 'Jacob', 'KGB', 'jackboot', 'Jacobi', 'cookbook', 'Jacobs', \"KGB's\", \"Jacob's\"]\n",
      "1 jukebox\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jukebox'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_check_int.check_spell('jukeboc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Use right dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theater'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_spell_ckeck_inst = spell_checker('en_US')\n",
    "us_spell_ckeck_inst.check_spell('theater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['theatre', 'heater', 'cheater', 'theta', 'that', 'eater', 'hater', 'tater', 'threader', 'beater', 'header', 'neater', 'teeter', 'Theiler']\n",
      "2 theatre\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'theatre'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br_spell_ckeck_inst = spell_checker('en_GB')\n",
    "br_spell_ckeck_inst.check_spell('theater')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Adding custom word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning\n",
      "nlp\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo -e \"deeplearning\\nnlp\" > my_words.txxxt\n",
    "cat my_words.txxxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = enchant.Dict('en_US')\n",
    "d1.check('nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = enchant.DictWithPWL ('en_US', 'my_words.txxxt')\n",
    "d2.check('nlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class synonynm(object):\n",
    "    def __init__(self, word_map):\n",
    "        self._map = word_map\n",
    "    def get_synonym(self, word):\n",
    "        return self._map.get(word, word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "synonynm_inst = synonynm({'bday': 'birthday', 'yolo':'you live only once'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you live only once'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonynm_inst.get_synonym('yolo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "we could have maintained a dictionary but this solution is not a extensible solution. One can maintain synonym dictionary in any format and synonym class can acts a wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "class csv_based_synonym(synonynm):\n",
    "    def __init__(self, file_name):\n",
    "        word_map = {}\n",
    "        for line in csv.reader(open(file_name)):\n",
    "            word, syn = line\n",
    "            word_map[word] = syn\n",
    "        super(csv_based_synonym ,self).__init__(word_map)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpy, happy\n",
      "bday, birthday\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Let's create a csv file\n",
    "echo -e 'hpy, happy\\nbday, birthday' > syn.csv\n",
    "cat syn.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' happy'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_based_synonym_int = csv_based_synonym('syn.csv')\n",
    "csv_based_synonym_int.get_synonym('hpy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Replacing negation with antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# review of WordNet(a lexical database for the English language)\n",
    "\n",
    "nltk provides an interface to WordNet synset(synonymous words that express the same concept.) lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('hike.n.01'),\n",
       " Synset('rise.n.09'),\n",
       " Synset('raise.n.01'),\n",
       " Synset('hike.v.01'),\n",
       " Synset('hike.v.02')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('hike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hike.n.01\n",
      "a long walk usually for exercise or pleasure\n",
      "rise.n.09\n",
      "an increase in cost\n",
      "raise.n.01\n",
      "the amount a salary is increased\n",
      "hike.v.01\n",
      "increase\n",
      "hike.v.02\n",
      "walk a long way, as for pleasure or physical exercise\n"
     ]
    }
   ],
   "source": [
    "for syn in wordnet.synsets('hike'):\n",
    "    print(syn.name())\n",
    "    print(syn.definition())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Looking for lemmas and synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('science.n.01'), Synset('skill.n.02')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take first synset for science\n",
    "syn = wordnet.synsets('science')[0]\n",
    "syn\n",
    "wordnet.synsets('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('science.n.01.science'), Lemma('science.n.01.scientific_discipline')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['science', 'scientific_discipline']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can treat lemmas as synonyms\n",
    "[l.name() for l in syn.lemmas()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Antonyms\n",
    "lemmas has antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('gladiolus.n.01'),\n",
       " Synset('glad.a.01'),\n",
       " Synset('glad.s.02'),\n",
       " Synset('glad.s.03'),\n",
       " Synset('beaming.s.01')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('glad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'showing or causing joy and pleasure; especially made happy'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = wordnet.synsets('glad')[1]\n",
    "syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('sad.a.01.sad')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glad_antonyms = syn.lemmas()[0].antonyms()\n",
    "glad_antonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiencing or showing sorrow or unhappiness; ; - Christina Rossetti'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glad_antonyms[0].synset().definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back to replacing negation with antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class antonym_replacer(object):\n",
    "    def _find_antonym(self, word, pos=None):\n",
    "        antonyms = set()\n",
    "        \n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lem in syn.lemmas():\n",
    "                for ant in lem.antonyms():\n",
    "                    antonyms.add(ant.name())\n",
    "        if len(antonyms) == 1:\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    def remove_negation(self, sent):\n",
    "        s=0\n",
    "        l=len(sent)\n",
    "        clean_words= []\n",
    "        while s < l -1:\n",
    "            possible_not_word = sent[s]\n",
    "            word = sent[s +1 ]\n",
    "            if possible_not_word == 'not':\n",
    "                ant= self._find_antonym(word)\n",
    "                print(ant)\n",
    "                if ant:\n",
    "                    clean_words.append(ant)\n",
    "                    s+=2\n",
    "            else:\n",
    "                clean_words.append(possible_not_word)\n",
    "                \n",
    "                if s==l-2:\n",
    "                    clean_words.append(word)\n",
    "                s+=1\n",
    "            \n",
    "        return clean_words    \n",
    "                                     \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'s\", 'not', 'uglify', 'this', 'place']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Let's not uglify this place\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's\", 'not', 'uglify', 'this', 'place']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What if we want Let's together\n",
    "regex= nltk.RegexpTokenizer(r'\\s+' , gaps=True)\n",
    "regex.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautify\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Let', \"'s\", 'beautify', 'this', 'place']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antonym_replacer_inst = antonym_replacer()\n",
    "antonym_replacer_inst.remove_negation(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Side: WordNet Methods you may find useful for your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Synstes are organised in the form of a tree using **hypernyms** and **hyponyms**\n",
    "\n",
    "**hypernyms:** abstract terms are known as hypernyms\n",
    "\n",
    "**hyponyms:** more specific terms as hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hike.n.01\n",
      "a long walk usually for exercise or pleasure\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('walk.n.04')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = wordnet.synsets('hike')[0]\n",
    "print(syn.name())\n",
    "print(syn.definition())\n",
    "syn.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('amble.n.01'),\n",
       " Synset('constitutional.n.01'),\n",
       " Synset('foot.n.07'),\n",
       " Synset('hike.n.01'),\n",
       " Synset('last_mile.n.01'),\n",
       " Synset('moonwalk.n.02'),\n",
       " Synset('perambulation.n.01'),\n",
       " Synset('turn.n.12'),\n",
       " Synset('walk-through.n.04'),\n",
       " Synset('walkabout.n.03')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('abstraction.n.06'),\n",
       "  Synset('psychological_feature.n.01'),\n",
       "  Synset('event.n.01'),\n",
       "  Synset('act.n.02'),\n",
       "  Synset('action.n.01'),\n",
       "  Synset('change.n.03'),\n",
       "  Synset('motion.n.06'),\n",
       "  Synset('travel.n.01'),\n",
       "  Synset('walk.n.04'),\n",
       "  Synset('hike.n.01')]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernym_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finding similarity\n",
    "Using hypernym tree for similarity between the Synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Wu-Palmer Similarity\n",
    "\n",
    "It is based on how similar the word senses are and realtive position of synsets in hypernym tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('faux_pas.n.01'),\n",
       " Synset('slip.n.02'),\n",
       " Synset('slip.n.03'),\n",
       " Synset('cutting.n.02'),\n",
       " Synset('slip.n.05'),\n",
       " Synset('mooring.n.01'),\n",
       " Synset('slip.n.07'),\n",
       " Synset('slickness.n.03'),\n",
       " Synset('strip.n.02'),\n",
       " Synset('slip.n.10'),\n",
       " Synset('chemise.n.01'),\n",
       " Synset('case.n.19'),\n",
       " Synset('skid.n.03'),\n",
       " Synset('slip.n.14'),\n",
       " Synset('slip.n.15'),\n",
       " Synset('steal.v.02'),\n",
       " Synset('slip.v.02'),\n",
       " Synset('skid.v.04'),\n",
       " Synset('slip.v.04'),\n",
       " Synset('slip.v.05'),\n",
       " Synset('err.v.01'),\n",
       " Synset('slip.v.07'),\n",
       " Synset('slip.v.08'),\n",
       " Synset('slip.v.09'),\n",
       " Synset('slip.v.10'),\n",
       " Synset('dislocate.v.01')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns = wordnet.synsets('slip')\n",
    "syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8235294117647058"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.wup_similarity(syns[0], syns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11764705882352941"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.wup_similarity(syns[0], wordnet.synsets('apple')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Above metric uses shortest path distance between the two Synsets and their common hypernym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Synset('faux_pas.n.01'), Synset('slip.n.02'))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = syns[0]\n",
    "d2 = syns[1]\n",
    "d1,d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.shortest_path_distance(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('blunder.n.01')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernym_d1 = d1.hypernyms()[0]\n",
    "common_hypernym_d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernym_d1.shortest_path_distance(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('mistake.n.01')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernym_d2 = d2.hypernyms()[0]\n",
    "common_hypernym_d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernym_d2.shortest_path_distance(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Be careful comparing verbs, as many verbs don't share common hypernyms. Return value would be None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words :\n",
    "\n",
    "a bag of words approach is used to perform tasks like document classification, i.e. classify articles as sports, science, health, etc...\n",
    "\n",
    "Some technical terms :\n",
    "- Document : phrase, paragraph, an entire article\n",
    "- Corpus : ensemble of documents \n",
    "\n",
    "This techniques disregards grammar and word order but maintains term multiplicity, i.e. number of occurrences of terms present. To implement a bag of words model we need to find the vocabulary of the corpus, and calculate the counts each word in our vocabulary appears in each document. This will result in a sparce matrix representation of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the steps :\n",
    "\n",
    "- tokenize\n",
    "- count occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here are the documents we are working with :\n",
    "docA = 'There is a snake in my boot'\n",
    "docB = 'There is no place like home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There', 'is', 'a', 'snake', 'in', 'my', 'boot']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We starts by tokenizing the documents :\n",
    "bowA = docA.split() \n",
    "bowB = docB.split()\n",
    "\n",
    "bowA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'There',\n",
       " 'a',\n",
       " 'boot',\n",
       " 'home',\n",
       " 'in',\n",
       " 'is',\n",
       " 'like',\n",
       " 'my',\n",
       " 'no',\n",
       " 'place',\n",
       " 'snake'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We get our vocabulary from our corpus :\n",
    "vocab = set(bowA).union(set(bowB))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'home': 0,\n",
       " 'my': 0,\n",
       " 'no': 0,\n",
       " 'boot': 0,\n",
       " 'is': 0,\n",
       " 'a': 0,\n",
       " 'in': 0,\n",
       " 'There': 0,\n",
       " 'like': 0,\n",
       " 'place': 0,\n",
       " 'snake': 0}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create dictionaries to compute the word count :\n",
    "docADict = dict.fromkeys(vocab, 0)\n",
    "docBDict = dict.fromkeys(vocab, 0)\n",
    "\n",
    "docBDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'home': 0,\n",
       " 'my': 1,\n",
       " 'no': 0,\n",
       " 'boot': 1,\n",
       " 'is': 1,\n",
       " 'a': 1,\n",
       " 'in': 1,\n",
       " 'There': 1,\n",
       " 'like': 0,\n",
       " 'place': 0,\n",
       " 'snake': 1}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We're ready to get the counts in of words in our documents :\n",
    "for w in bowA:\n",
    "    docADict[w] += 1\n",
    "\n",
    "for w in bowB:\n",
    "    docBDict[w] += 1\n",
    "\n",
    "#Check the counts\n",
    "docADict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home</th>\n",
       "      <th>my</th>\n",
       "      <th>no</th>\n",
       "      <th>boot</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>in</th>\n",
       "      <th>There</th>\n",
       "      <th>like</th>\n",
       "      <th>place</th>\n",
       "      <th>snake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   home  my  no  boot  is  a  in  There  like  place  snake\n",
       "0     0   1   0     1   1  1   1      1     0      0      1\n",
       "1     1   0   1     0   1  0   0      1     1      1      0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dataframe for easier read :\n",
    "pd.DataFrame([docADict,docBDict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, bag of words is something that has been around for a while and there are packages in Python that will take care of all of this for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create our CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#This is the corpus\n",
    "corpus = ['This is the text of the first document.', \n",
    "          'This is the second document text with a longer sentence.', \n",
    "          'Number three.', \n",
    "          'This is number four.'] \n",
    "\n",
    "#Let's create and fit a CountVectorizer object on the corpus\n",
    "vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in our corpus : 14\n",
      "The vocabulary : ['document', 'first', 'four', 'is', 'longer', 'number', 'of', 'second', 'sentence', 'text', 'the', 'this', 'three', 'with']\n"
     ]
    }
   ],
   "source": [
    "#Let's look at the vocabulary of our corpus\n",
    "print(\"Number of unique words in our corpus :\",len(vectorizer.get_feature_names()))\n",
    "print(\"The vocabulary :\",vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary : {'this': 11, 'is': 3, 'the': 10, 'text': 9, 'of': 6, 'first': 1, 'document': 0, 'second': 7, 'with': 13, 'longer': 4, 'sentence': 8, 'number': 5, 'three': 12, 'four': 2}\n"
     ]
    }
   ],
   "source": [
    "#We can also look at the dictionary which maps each vocabulary word to its location :\n",
    "print(\"The vocabulary :\",vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 10)\t2\n",
      "  (0, 9)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 8)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 12)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "# Let's create out bag of words\n",
    "BoW = vectorizer.fit_transform(corpus)\n",
    "print(BoW)\n",
    "\n",
    "#First entry is the document. \n",
    "#Second entry is the index of the word\n",
    "#The value is the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary : ['document', 'first', 'four', 'is', 'longer', 'number', 'of', 'second', 'sentence', 'text', 'the', 'this', 'three', 'with']\n",
      "[[1 1 0 1 0 0 1 0 0 1 2 1 0 0]\n",
      " [1 0 0 1 1 0 0 1 1 1 1 1 0 1]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 1 0 1 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the matrix in the numpy form\n",
    "print(\"The vocabulary :\",vectorizer.get_feature_names())\n",
    "print(BoW.toarray())\n",
    "\n",
    "# Sanity check :\n",
    "# 'This is the text of the first document.'\n",
    "# 'This is the second document text with a longer sentence.' \n",
    "# 'Number three.'\n",
    "# 'This is number four.'\n",
    "\n",
    "\n",
    "# Term-Document matrix\n",
    "# Word vector\n",
    "# Document vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>four</th>\n",
       "      <th>is</th>\n",
       "      <th>longer</th>\n",
       "      <th>number</th>\n",
       "      <th>of</th>\n",
       "      <th>second</th>\n",
       "      <th>sentence</th>\n",
       "      <th>text</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>three</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document  first  four  is  longer  number  of  second  sentence  text  the  \\\n",
       "0         1      1     0   1       0       0   1       0         0     1    2   \n",
       "1         1      0     0   1       1       0   0       1         1     1    1   \n",
       "2         0      0     0   0       0       1   0       0         0     0    0   \n",
       "3         0      0     1   1       0       1   0       0         0     0    0   \n",
       "\n",
       "   this  three  with  \n",
       "0     1      0     0  \n",
       "1     1      0     1  \n",
       "2     0      1     0  \n",
       "3     1      0     0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can of course create a pandas DataFrame with this information and have a cleaner way to view our data\n",
    "pd.DataFrame(BoW.todense(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the vocabulary learned to transform a new document\n",
    "vectorizer.transform(['My new document.']).toarray()\n",
    "\n",
    "#[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at a movie review classification example from Kaggle\n",
    "\n",
    "Data can be found [here](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's import some movie reviews :\n",
    "movie_review = pd.read_csv('IMDB Dataset.csv')\n",
    "movie_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        0\n",
       "4        1\n",
       "        ..\n",
       "49995    1\n",
       "49996    0\n",
       "49997    0\n",
       "49998    0\n",
       "49999    0\n",
       "Name: sentiment, Length: 50000, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's first map positive/negative label to a numberic value\n",
    "movie_review.sentiment = movie_review.sentiment.map({\"positive\":1, \"negative\":0})\n",
    "movie_review.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    25000\n",
       "0    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's look at how many positive/negative reviews we have :\n",
    "movie_review.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package to split the data into training/testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Let's split our data into train/test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    movie_review['review'], \n",
    "    movie_review['sentiment'], \n",
    "    test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000,) (15000,) (35000,) (15000,)\n"
     ]
    }
   ],
   "source": [
    "#Check out our train and test data sets\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the bag of words object :\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's perform our bag of words model fit on the train and test data :\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "#Why do we only transform the X_test data ?...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88339"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The size of out vocabulary is :\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['polygamy',\n",
       " 'polygon',\n",
       " 'polygram',\n",
       " 'polygraph',\n",
       " 'polymath',\n",
       " 'polymer',\n",
       " 'polynesia',\n",
       " 'polynesian',\n",
       " 'polyphobia',\n",
       " 'polystyrene',\n",
       " 'polysyllabic',\n",
       " 'polytheism',\n",
       " 'polytheistic',\n",
       " 'polytheists',\n",
       " 'pom',\n",
       " 'pomade',\n",
       " 'pomegranate',\n",
       " 'pomegranates',\n",
       " 'pomeii',\n",
       " 'pomeranian',\n",
       " 'pomerantz',\n",
       " 'pomeranz',\n",
       " 'pommel',\n",
       " 'pommies',\n",
       " 'pomo',\n",
       " 'pomp',\n",
       " 'pompadour',\n",
       " 'pompadoured',\n",
       " 'pompadours',\n",
       " 'pompeii',\n",
       " 'pompeo',\n",
       " 'pompom',\n",
       " 'pomposity',\n",
       " 'pompous',\n",
       " 'pompously',\n",
       " 'pompousness',\n",
       " 'poms',\n",
       " 'pon',\n",
       " 'poncelet',\n",
       " 'poncho',\n",
       " 'pond',\n",
       " 'ponder',\n",
       " 'ponderance',\n",
       " 'pondered',\n",
       " 'pondering',\n",
       " 'ponderosa',\n",
       " 'ponderous',\n",
       " 'ponderously',\n",
       " 'ponders',\n",
       " 'ponds',\n",
       " 'pone',\n",
       " 'ponente',\n",
       " 'pong',\n",
       " 'pongo',\n",
       " 'pongs',\n",
       " 'ponied',\n",
       " 'ponies',\n",
       " 'pons',\n",
       " 'pont',\n",
       " 'ponte',\n",
       " 'pontecorvo',\n",
       " 'pontevedro',\n",
       " 'ponti',\n",
       " 'pontiac',\n",
       " 'pontiacs',\n",
       " 'pontificate',\n",
       " 'pontificates',\n",
       " 'pontificating',\n",
       " 'pontification',\n",
       " 'pontifications',\n",
       " 'pontificator',\n",
       " 'pontin',\n",
       " 'pontins',\n",
       " 'pontius',\n",
       " 'ponto',\n",
       " 'pontoon',\n",
       " 'ponty',\n",
       " 'pontypool',\n",
       " 'pony',\n",
       " 'ponygon',\n",
       " 'ponyo',\n",
       " 'ponyos',\n",
       " 'ponytail',\n",
       " 'ponytails',\n",
       " 'poo',\n",
       " 'pooa',\n",
       " 'pooch',\n",
       " 'pooches',\n",
       " 'poodle',\n",
       " 'poodles',\n",
       " 'pooed',\n",
       " 'poof',\n",
       " 'poofed',\n",
       " 'poofs',\n",
       " 'poofters',\n",
       " 'poofy',\n",
       " 'pooh',\n",
       " 'pooja',\n",
       " 'pookal',\n",
       " 'pool']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's look at a few of the vocabulary words we have in our reviews :\n",
    "vectorizer.get_feature_names()[60000:60100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Let's import a few packages that will allow us to perform a logistic regression and evaluate its performance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(),X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our average accuracy is : 88.34\n"
     ]
    }
   ],
   "source": [
    "#Looking at the average accuracy score of the different cross validation we get :\n",
    "print(\"Our average accuracy is :\", round(np.mean(scores)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since we split our data into training and testing sets, let's build a model, train it on the train data\n",
    "#and then use it to predict the sentiment of the movie reviews in the test data set :\n",
    "movie_logreg = LogisticRegression()\n",
    "movie_logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training score was: 99.77\n",
      "The testing score was: 88.74\n"
     ]
    }
   ],
   "source": [
    "#Check the accuracy of the model on the \n",
    "print(\"The training score was:\", round(movie_logreg.score(X_train,y_train)*100,2))\n",
    "\n",
    "print(\"The testing score was:\", round(movie_logreg.score(X_test,y_test)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>6605</td>\n",
       "      <td>916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>773</td>\n",
       "      <td>6706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Positive  Negative\n",
       "Positive      6605       916\n",
       "Negative       773      6706"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now want to see the prediction our model makes with the review in the test set\n",
    "movie_pred = movie_logreg.predict(X_test)\n",
    "\n",
    "#By looking at the confusion matrix we get a nice summary of how our model performed :\n",
    "pd.DataFrame(confusion_matrix(y_test, movie_pred), \n",
    "             columns=[\"Positive\", \"Negative\"], \n",
    "             index=[\"Positive\", \"Negative\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF : Term Frequency Inverse Document Frequency\n",
    "\n",
    "As great as bag of words is, it takes into account a lot of words that don't carry a lot of meaning that may occur a lot in most or all documents we are using. We want a way to avoid giving a lot of importance to words that may not help our analysis.\n",
    "\n",
    "A few key words to know :\n",
    "\n",
    "- Term Frequency : is the number of times a word appears in a document, divided by the total number of words in said document. Just counting the number of occurrences would give more weight to longer documents, so we are looking at relative counts.\n",
    "- Document Frequency : is the number of documents in which the word is present\n",
    "- Inverse Document Frequency : it measures the informativeness of the word w. IDF will give a very low value to the most occurring words like stopwords (yes we eliminate stopwords in the preprocessing step but remember that there is no definitive set of stopwords).\n",
    "\n",
    "Instead of counting word occurrences we want to calculate the TF-IDF score for words, which is a score a score that represents word importance :\n",
    "\n",
    "$$tf(w)*idf (w)$$\n",
    "\n",
    "where :\n",
    "\n",
    "$tf(w)$ : (total count of the word, w, in the document)/(total number of words in the document)\n",
    "\n",
    "$idf(w)$ : log((number of documents)/number of documents that contain the word w)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Why do we divide the term frequency by the number of words in the document ?\n",
    "\n",
    "Why to we take the log in the idf term ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first calculate the term frequency of each word (this should remind you of bag of words above)\n",
    "def termFreq(wDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word,count in wDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'home': 0.16666666666666666,\n",
       " 'my': 0.0,\n",
       " 'no': 0.16666666666666666,\n",
       " 'boot': 0.0,\n",
       " 'is': 0.16666666666666666,\n",
       " 'a': 0.0,\n",
       " 'in': 0.0,\n",
       " 'There': 0.16666666666666666,\n",
       " 'like': 0.16666666666666666,\n",
       " 'place': 0.16666666666666666,\n",
       " 'snake': 0.0}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdocA = termFreq(docADict, bowA)\n",
    "tfdocB = termFreq(docBDict, bowB)\n",
    "\n",
    "tfdocB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def computeIDF(listDocs):\n",
    "    #Let's get the number of documents in our corpus\n",
    "    totDocs = len(listDocs)\n",
    "    \n",
    "    #Create a dictionary with our vocabulary and initialize values to 0\n",
    "    idfDict = dict.fromkeys(listDocs[0].keys(), 0)\n",
    "    \n",
    "    #Get a count of the number of documents that contain the word w\n",
    "    for doc in listDocs:\n",
    "        for w, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[w] += 1\n",
    "                \n",
    "    #we now divide totDocs by the number of documents the word appears in and take the log \n",
    "    for w, val in idfDict.items():\n",
    "        idfDict[w] = math.log(totDocs/float(val))\n",
    "    \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a snake in my boot\n",
      "There is no place like home\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'home': 0.6931471805599453,\n",
       " 'my': 0.6931471805599453,\n",
       " 'no': 0.6931471805599453,\n",
       " 'boot': 0.6931471805599453,\n",
       " 'is': 0.0,\n",
       " 'a': 0.6931471805599453,\n",
       " 'in': 0.6931471805599453,\n",
       " 'There': 0.0,\n",
       " 'like': 0.6931471805599453,\n",
       " 'place': 0.6931471805599453,\n",
       " 'snake': 0.6931471805599453}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(docA)\n",
    "print(docB)\n",
    "\n",
    "idfs = computeIDF([docADict,docBDict])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for w, val in tfBow.items():\n",
    "        tfidf[w] = val*idfs[w]\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfBowA = calcTFIDF(tfdocA, idfs)\n",
    "tfidfBowB = calcTFIDF(tfdocB, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home</th>\n",
       "      <th>my</th>\n",
       "      <th>no</th>\n",
       "      <th>boot</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>in</th>\n",
       "      <th>There</th>\n",
       "      <th>like</th>\n",
       "      <th>place</th>\n",
       "      <th>snake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       home        my        no      boot   is         a        in  There  \\\n",
       "0  0.000000  0.099021  0.000000  0.099021  0.0  0.099021  0.099021    0.0   \n",
       "1  0.115525  0.000000  0.115525  0.000000  0.0  0.000000  0.000000    0.0   \n",
       "\n",
       "       like     place     snake  \n",
       "0  0.000000  0.000000  0.099021  \n",
       "1  0.115525  0.115525  0.000000  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([tfidfBowA,tfidfBowB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "firstDoc = 'the lady went took the dog for a walk'\n",
    "secondDoc = 'the children roasted marshmellows around the fire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.35300279 0.         0.35300279 0.35300279\n",
      "  0.         0.         0.50232878 0.35300279 0.35300279 0.35300279]\n",
      " [0.37729199 0.37729199 0.         0.37729199 0.         0.\n",
      "  0.37729199 0.37729199 0.53689271 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Get built-in TF-IDF method\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#This steps learns the vocabulary in the documents and returns the term-document matrix\n",
    "vectors = vectorizer.fit_transform([firstDoc, secondDoc])\n",
    "# print(vectors)\n",
    "\n",
    "#This extracts the vocabulary learned. Used to create the dataframe later.\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "# print(feature_names)\n",
    "\n",
    "#This creates a dense matrix of the term-document matrix\n",
    "dense = vectors.todense()\n",
    "print(dense)\n",
    "\n",
    "#Make it into a list so we can create the dataframe\n",
    "denselist = dense.tolist()\n",
    "\n",
    "#Store results\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>around</th>\n",
       "      <th>children</th>\n",
       "      <th>dog</th>\n",
       "      <th>fire</th>\n",
       "      <th>for</th>\n",
       "      <th>lady</th>\n",
       "      <th>marshmellows</th>\n",
       "      <th>roasted</th>\n",
       "      <th>the</th>\n",
       "      <th>took</th>\n",
       "      <th>walk</th>\n",
       "      <th>went</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502329</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.353003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.377292</td>\n",
       "      <td>0.377292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377292</td>\n",
       "      <td>0.377292</td>\n",
       "      <td>0.536893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     around  children       dog      fire       for      lady  marshmellows  \\\n",
       "0  0.000000  0.000000  0.353003  0.000000  0.353003  0.353003      0.000000   \n",
       "1  0.377292  0.377292  0.000000  0.377292  0.000000  0.000000      0.377292   \n",
       "\n",
       "    roasted       the      took      walk      went  \n",
       "0  0.000000  0.502329  0.353003  0.353003  0.353003  \n",
       "1  0.377292  0.536893  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit and Latent Semantic Analysis \n",
    "\n",
    "Explicit and Latent semantic analisys are quite similar with the key difference being that ESA, the concepts are already known and have been labeled, the concepts are considered to already be \"manifested\". ESA uses prior knowledge of realtionships between words and concepts.\n",
    "\n",
    "LSA on the other hand does not use prior knowledge and therefore \"discovers\" the concepts of the data.\n",
    "\n",
    "We will focus on Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA :\n",
    "\n",
    "For LSA we will need to feed to model a matrix with dimensions m x n ; where m is the number of documents in the corpus and n in the number of words in our vocabulary.\n",
    "\n",
    "In order to perform the LSA and uncover the concepts hidden in our corpus, we first have to think about how many concepts, k, we want to extract. Since we are performing LSA, we don't know how many concepts there are, therefore the number of concepts is what is called a hyperparameter. \n",
    "- A hyperparameter is a parameter that has to be set and fine tuned by the user "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract the concepts that are hidden in our corpus we have to perform a Singular Value Decomposition (SVD). \n",
    "\n",
    "$$X \\approx U\\Sigma V^T$$\n",
    "\n",
    "where the different matrices in the SVD decomposition are :\n",
    "- $U$ : is an m x k matrix. The rows are the documents in our corpus and the columns are the 'concepts'. It is the document to concept similarity matrix.\n",
    "- $\\Sigma$ : is a k x k matrix. Each value represents the strength of each concept. \n",
    "- $V$ : is an n x k matrix (there's a transpose so the dimensions work out). The rows are the terms and the columns are the concepts.\n",
    "\n",
    "A neat video explaining SVD is available [here](https://www.youtube.com/watch?v=P5mlg91as1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at Latent Semantic Analysis (LSA) with a neat example from Kaggle found [here](https://www.kaggle.com/rcushen/topic-modelling-with-lsa-and-lda/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publish_date                                      headline_text\n",
       "0   2003-02-19  aba decides against community broadcasting lic...\n",
       "1   2003-02-19     act fire witnesses must be aware of defamation\n",
       "2   2003-02-19     a g calls for infrastructure protection summit\n",
       "3   2003-02-19           air nz staff in aust strike for pay rise\n",
       "4   2003-02-19      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in the data\n",
    "raw_data = pd.read_csv('abcnews-date-text.csv', parse_dates=[0], infer_datetime_format=True)\n",
    "\n",
    "#Extract the information that we are interested in and reindex it\n",
    "reindexed_data = raw_data['headline_text']\n",
    "reindexed_data.index = raw_data['publish_date']\n",
    "\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline before vectorization: emerging springboks giant joins tahs\n",
      "Headline after vectorization: \n",
      "  (0, 3813)\t1\n",
      "  (0, 10457)\t1\n",
      "  (0, 4763)\t1\n",
      "  (0, 5979)\t1\n",
      "  (0, 10926)\t1\n"
     ]
    }
   ],
   "source": [
    "#We create the term-document matrix, remove stopwords and limit the vocabulary to 40000 terms\n",
    "small_count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\n",
    "#Extract a small sample :\n",
    "small_text_sample = reindexed_data.sample(n=10000, random_state=0).values\n",
    "\n",
    "print('Headline before vectorization: {}'.format(small_text_sample[123]))\n",
    "\n",
    "#We apply the transformation to our sample data set :\n",
    "small_document_term_matrix = small_count_vectorizer.fit_transform(small_text_sample)\n",
    "print('Headline after vectorization: \\n{}'.format(small_document_term_matrix[123]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we select an arbitrary number of topics (concepts) we want to uncover :\n",
    "n_topics = 8 #this is out hyperparameter k\n",
    "\n",
    "#The truncated SVD performs the dimensionality reduction of our data and will extract 8 concepts from \n",
    "#our corpus\n",
    "lsa_model = TruncatedSVD(n_components=n_topics)\n",
    "\n",
    "#We fit the LSA model to our data :\n",
    "lsa_topic_matrix = lsa_model.fit_transform(small_document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_keys(topic_matrix):\n",
    "    '''\n",
    "    returns an integer list of predicted topic \n",
    "    categories for a given topic matrix\n",
    "    '''\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "\n",
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We extract the most likely concepts that a particular document belongs to :\n",
    "lsa_keys = get_keys(lsa_topic_matrix)\n",
    "\n",
    "#We extract the categories and their respective counts :\n",
    "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
    "    '''\n",
    "    returns a list of n_topic strings, where each string contains the n most common \n",
    "    words in a predicted category, in order\n",
    "    '''\n",
    "    top_word_indices = []\n",
    "    for topic in range(n_topics):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "        \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:,index] = 1\n",
    "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "        top_words.append(\" \".join(topic_words))         \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:  police crash probe car drug missing fatal search woman road\n",
      "Topic 2:  man charged murder dies jailed court accused guilty bail arrested\n",
      "Topic 3:  new laws years year open abc queensland weather trial sets\n",
      "Topic 4:  says health wa report australian government help group nsw minister\n",
      "Topic 5:  court face high accused sex charges told ban faces trial\n",
      "Topic 6:  govt hospital urged sa vic work act plan funds boost\n",
      "Topic 7:  council plan election water backs takes centre fears business wins\n",
      "Topic 8:  interview australia world coast win cup china gold south nsw\n"
     ]
    }
   ],
   "source": [
    "#We extract the top 10 words that belong to each topic :\n",
    "top_n_words_lsa = get_top_n_words(10, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "\n",
    "#Look at the top 10 words for each category :\n",
    "for i in range(len(top_n_words_lsa)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# read nltk for word collocation\n",
    "\n",
    "- http://www.nltk.org/howto/collocations.html\n",
    "- https://www.nltk.org/\n",
    "\n",
    "- https://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# NLP Resources\n",
    "https://web.stanford.edu/~jurafsky/slp3/\n",
    "\n",
    "http://web.stanford.edu/class/cs224n/\n",
    "\n",
    "1 - Ruder website: http://ruder.io/ (all his tutorials are amazing, I suggest you to start from old posts he has on the website)\n",
    "\n",
    "2 - https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8416973&tag=1 (this covers the most recent advances in DL in NLP)\n",
    "\n",
    "3 - pretty much everything in this website: https://machinelearningmastery.com/category/natural-language-processing/\n",
    "\n",
    "4 - This github repo has a lot of good resources: https://github.com/keon/awesome-nlp\n",
    "\n",
    "5- https://www.youtube.com/watch?v=jfwqRMdTmLo&list=\n",
    "\n",
    "6- For those of you wanting a nice summery and some deep learning related to NLP, check [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
